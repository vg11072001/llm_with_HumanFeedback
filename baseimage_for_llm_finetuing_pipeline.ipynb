{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 66631,
          "databundleVersionId": 8346466,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30887,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d73f3c02e2a446a98345bf3baa24662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff707e3e3d6a42bdb5da9856ba0ad568"
            ],
            "layout": "IPY_MODEL_e309f72a81784542953074f4de90e844"
          }
        },
        "851b4c7a69db457d9366f7eb88b72412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f132822ceb84514892ed9a78fdee6df",
            "placeholder": "​",
            "style": "IPY_MODEL_dd15a928b22f44808c546973bb3fe280",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "5b0ad053e58148fca04cca50e17ced3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_eb2f3c67a3404fb197b97f6affb907dc",
            "placeholder": "​",
            "style": "IPY_MODEL_c911be4832a64ce7a030e3db7d8dcd15",
            "value": "vanshikagupta1136"
          }
        },
        "d8b4f96261454d33927060ec6384ce68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_e871ace70f05423fb7ea67d34fbd70d4",
            "placeholder": "​",
            "style": "IPY_MODEL_33afa8bd7c684c5da53016b03df3aaf1",
            "value": ""
          }
        },
        "065b4e097061462d9ebd06bdcfc053b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_cb1563f12db84a8b81e4fd71ba065b47",
            "style": "IPY_MODEL_1f2990e77d764f2bbccfefe0bd5b11fc",
            "tooltip": ""
          }
        },
        "dffe0d79d9a34c2ca0aaf7b138da4513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a896165da0b74194ba9f74a0a9b6455c",
            "placeholder": "​",
            "style": "IPY_MODEL_b3496a5249d548f19822811f063858ce",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "e309f72a81784542953074f4de90e844": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "9f132822ceb84514892ed9a78fdee6df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd15a928b22f44808c546973bb3fe280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb2f3c67a3404fb197b97f6affb907dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c911be4832a64ce7a030e3db7d8dcd15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e871ace70f05423fb7ea67d34fbd70d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33afa8bd7c684c5da53016b03df3aaf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb1563f12db84a8b81e4fd71ba065b47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f2990e77d764f2bbccfefe0bd5b11fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "a896165da0b74194ba9f74a0a9b6455c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3496a5249d548f19822811f063858ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "998ba7dbb7fa4c43b4e7b334910b3b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb4afde7ac6e44edaea9002a03184f31",
            "placeholder": "​",
            "style": "IPY_MODEL_a1021ed77042472baef6df9b6948c3e5",
            "value": "Connecting..."
          }
        },
        "eb4afde7ac6e44edaea9002a03184f31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1021ed77042472baef6df9b6948c3e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff707e3e3d6a42bdb5da9856ba0ad568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c81e30d441a47cf9e8ffad2e94754f9",
            "placeholder": "​",
            "style": "IPY_MODEL_807bf19d4e6541f3a9843851946c18e7",
            "value": "Kaggle credentials successfully validated."
          }
        },
        "4c81e30d441a47cf9e8ffad2e94754f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "807bf19d4e6541f3a9843851946c18e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vg11072001/llm_with_HumanFeedback/blob/main/baseimage_for_llm_finetuing_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()"
      ],
      "metadata": {
        "id": "VYM_plpTy66l",
        "outputId": "e11d920a-20b7-4441-9a8c-f88dd3960dd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "4d73f3c02e2a446a98345bf3baa24662",
            "851b4c7a69db457d9366f7eb88b72412",
            "5b0ad053e58148fca04cca50e17ced3a",
            "d8b4f96261454d33927060ec6384ce68",
            "065b4e097061462d9ebd06bdcfc053b2",
            "dffe0d79d9a34c2ca0aaf7b138da4513",
            "e309f72a81784542953074f4de90e844",
            "9f132822ceb84514892ed9a78fdee6df",
            "dd15a928b22f44808c546973bb3fe280",
            "eb2f3c67a3404fb197b97f6affb907dc",
            "c911be4832a64ce7a030e3db7d8dcd15",
            "e871ace70f05423fb7ea67d34fbd70d4",
            "33afa8bd7c684c5da53016b03df3aaf1",
            "cb1563f12db84a8b81e4fd71ba065b47",
            "1f2990e77d764f2bbccfefe0bd5b11fc",
            "a896165da0b74194ba9f74a0a9b6455c",
            "b3496a5249d548f19822811f063858ce",
            "998ba7dbb7fa4c43b4e7b334910b3b56",
            "eb4afde7ac6e44edaea9002a03184f31",
            "a1021ed77042472baef6df9b6948c3e5",
            "ff707e3e3d6a42bdb5da9856ba0ad568",
            "4c81e30d441a47cf9e8ffad2e94754f9",
            "807bf19d4e6541f3a9843851946c18e7"
          ]
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d73f3c02e2a446a98345bf3baa24662"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle credentials set.\n",
            "Kaggle credentials successfully validated.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "lmsys_chatbot_arena_path = kagglehub.competition_download('lmsys-chatbot-arena')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Vw9dHQfUy66q",
        "outputId": "8f36b1c5-5f0d-4ca9-8c16-263bba67cbb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/competitions/data/download-all/lmsys-chatbot-arena...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 57.0M/57.0M [00:00<00:00, 95.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "4b10bca5-5b51-474f-a022-65c838836fb6",
        "_cell_guid": "2431e274-5283-456f-8919-f3a8d43cb939",
        "trusted": true,
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:02:30.534875Z",
          "iopub.execute_input": "2025-02-23T00:02:30.535131Z",
          "iopub.status.idle": "2025-02-23T00:02:30.915035Z",
          "shell.execute_reply.started": "2025-02-23T00:02:30.535079Z",
          "shell.execute_reply": "2025-02-23T00:02:30.914206Z"
        },
        "scrolled": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "5v1-SVI7y66s"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install kaggle_secrets"
      ],
      "metadata": {
        "id": "5juzCcDLznyn",
        "outputId": "85b220f9-4f71-4427-e339-176db003d1a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement kaggle_secrets (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for kaggle_secrets\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "import json\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "try:\n",
        "  from kaggle_secrets import UserSecretsClient\n",
        "  user_secrets = UserSecretsClient()\n",
        "  secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n",
        "except:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  from google.colab import userdata\n",
        "  secret_value_0 = userdata.get(\"HF_TOKEN\")\n",
        "except:\n",
        "  pass\n",
        "\n",
        "login(secret_value_0)"
      ],
      "metadata": {
        "_uuid": "b84d01aa-aead-4631-ba2c-e98f632b0a3c",
        "_cell_guid": "9680ab67-f52e-4316-8448-581508889e2e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:02:30.915918Z",
          "iopub.execute_input": "2025-02-23T00:02:30.916334Z",
          "iopub.status.idle": "2025-02-23T00:02:34.562869Z",
          "shell.execute_reply.started": "2025-02-23T00:02:30.9163Z",
          "shell.execute_reply": "2025-02-23T00:02:34.561731Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "d8i2f4rpy66t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "lmsys_chatbot_arena_path"
      ],
      "metadata": {
        "id": "aPSD-7Oa1b7v",
        "outputId": "493e67e3-2eb9-4a5f-97ce-1c52fc1d3809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/kagglehub/competitions/lmsys-chatbot-arena'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/root/.cache/kagglehub/competitions/lmsys-chatbot-arena/train.csv')\n",
        "test = pd.read_csv('/root/.cache/kagglehub/competitions/lmsys-chatbot-arena/test.csv')"
      ],
      "metadata": {
        "_uuid": "5beb49e1-eadd-481a-bad6-d75e2e581018",
        "_cell_guid": "c3dc0aa4-70d6-4cb1-98fb-66f825c6cb87",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:02:34.563854Z",
          "iopub.execute_input": "2025-02-23T00:02:34.564415Z",
          "iopub.status.idle": "2025-02-23T00:02:38.747486Z",
          "shell.execute_reply.started": "2025-02-23T00:02:34.564384Z",
          "shell.execute_reply": "2025-02-23T00:02:38.746583Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "9q0s6yDvy66v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preparation**"
      ],
      "metadata": {
        "_uuid": "e614db25-44db-4941-a0c2-7a17542611e6",
        "_cell_guid": "55da91c1-5792-427b-bf2b-5d005525cb9d",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "TWzmZIZ9y66w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----------train-----------')\n",
        "print(train.head(1))\n",
        "print(train.shape)\n",
        "print(train.columns)"
      ],
      "metadata": {
        "_uuid": "ab4c8304-1298-47ff-8e26-82875b0cb141",
        "_cell_guid": "516dac1c-892f-4c12-9f17-4d144e19ad05",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:02:38.749345Z",
          "iopub.execute_input": "2025-02-23T00:02:38.749615Z",
          "iopub.status.idle": "2025-02-23T00:02:38.769318Z",
          "shell.execute_reply.started": "2025-02-23T00:02:38.749593Z",
          "shell.execute_reply": "2025-02-23T00:02:38.767751Z"
        },
        "id": "hn3BCub5y66z",
        "outputId": "4f4d7f71-d139-44d9-c1d3-f954886406cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------train-----------\n",
            "      id             model_a     model_b  \\\n",
            "0  30192  gpt-4-1106-preview  gpt-4-0613   \n",
            "\n",
            "                                              prompt  \\\n",
            "0  [\"Is it morally right to try to have a certain...   \n",
            "\n",
            "                                          response_a  \\\n",
            "0  [\"The question of whether it is morally right ...   \n",
            "\n",
            "                                          response_b  winner_model_a  \\\n",
            "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
            "\n",
            "   winner_model_b  winner_tie  \n",
            "0               0           0  \n",
            "(57477, 9)\n",
            "Index(['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b',\n",
            "       'winner_model_a', 'winner_model_b', 'winner_tie'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "_uuid": "60f00ca6-d805-41b6-bc99-260a793bd486",
        "_cell_guid": "56f6a95c-755a-4105-a150-7f63f8e86674",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:02:38.77096Z",
          "iopub.execute_input": "2025-02-23T00:02:38.771345Z",
          "iopub.status.idle": "2025-02-23T00:02:38.835034Z",
          "shell.execute_reply.started": "2025-02-23T00:02:38.771304Z",
          "shell.execute_reply": "2025-02-23T00:02:38.833302Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "8Gc3s2EBy661",
        "outputId": "a142b973-789f-42ea-fb3f-25ed2e302b99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 57477 entries, 0 to 57476\n",
            "Data columns (total 9 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   id              57477 non-null  int64 \n",
            " 1   model_a         57477 non-null  object\n",
            " 2   model_b         57477 non-null  object\n",
            " 3   prompt          57477 non-null  object\n",
            " 4   response_a      57477 non-null  object\n",
            " 5   response_b      57477 non-null  object\n",
            " 6   winner_model_a  57477 non-null  int64 \n",
            " 7   winner_model_b  57477 non-null  int64 \n",
            " 8   winner_tie      57477 non-null  int64 \n",
            "dtypes: int64(4), object(5)\n",
            "memory usage: 3.9+ MB\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "group_id = train['prompt']\n",
        "label_id = train['winner_model_a winner_model_b winner_tie'.split()].values.argmax(1)\n",
        "splits = list(skf.split(train, label_id, group_id))"
      ],
      "metadata": {
        "_uuid": "421aadfc-a7e1-47b9-be88-6e4cce6a22e7",
        "_cell_guid": "7f90cb5c-4fb1-4092-9141-dab5adc532d8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:02:38.836011Z",
          "iopub.execute_input": "2025-02-23T00:02:38.836431Z",
          "iopub.status.idle": "2025-02-23T00:03:00.080271Z",
          "shell.execute_reply.started": "2025-02-23T00:02:38.836389Z",
          "shell.execute_reply": "2025-02-23T00:03:00.079357Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "aqKu26XMy663"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"group_id: {group_id}\")\n",
        "print(f\"label: {label_id}, len {len(label_id)}\")"
      ],
      "metadata": {
        "_uuid": "3a276caf-981b-4ac6-a9b0-f923d49289e4",
        "_cell_guid": "123047cc-8100-4f2b-ab9e-57ee75f0c730",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:00.081139Z",
          "iopub.execute_input": "2025-02-23T00:03:00.081477Z",
          "iopub.status.idle": "2025-02-23T00:03:00.088285Z",
          "shell.execute_reply.started": "2025-02-23T00:03:00.081445Z",
          "shell.execute_reply": "2025-02-23T00:03:00.086994Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "QIo2ZSUey663",
        "outputId": "d587dff1-2b62-4c49-fda9-a259b45ca5d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "group_id: 0        [\"Is it morally right to try to have a certain...\n",
            "1        [\"What is the difference between marriage lice...\n",
            "2        [\"explain function calling. how would you call...\n",
            "3        [\"How can I create a test set for a very rare ...\n",
            "4        [\"What is the best way to travel from Tel-Aviv...\n",
            "                               ...                        \n",
            "57472    [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...\n",
            "57473    [\"In python, implement a naive Bayes with gaus...\n",
            "57474    [\"is it unethical to work on building weapons?...\n",
            "57475    [\"If a bait contains 0,0025% bromadiolon then ...\n",
            "57476    [\"three kids eat three apples in three days, h...\n",
            "Name: prompt, Length: 57477, dtype: object\n",
            "label: [0 1 2 ... 0 1 0], len 57477\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "splits"
      ],
      "metadata": {
        "_uuid": "b9ce5510-1adc-4bd2-a236-f6d47635d3a0",
        "_cell_guid": "946db15a-c544-4c3f-ac2b-df4a77da570b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:00.089265Z",
          "iopub.execute_input": "2025-02-23T00:03:00.089548Z",
          "iopub.status.idle": "2025-02-23T00:03:00.108434Z",
          "shell.execute_reply.started": "2025-02-23T00:03:00.089522Z",
          "shell.execute_reply": "2025-02-23T00:03:00.107522Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "cio05qwLy664",
        "outputId": "6b0a1ecf-0357-49ac-c19d-d7c8f44d324f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(array([    0,     1,     2, ..., 57474, 57475, 57476]),\n",
              "  array([    3,    21,    33, ..., 57467, 57468, 57470])),\n",
              " (array([    0,     1,     2, ..., 57474, 57475, 57476]),\n",
              "  array([    4,     6,    12, ..., 57456, 57460, 57464])),\n",
              " (array([    0,     3,     4, ..., 57474, 57475, 57476]),\n",
              "  array([    1,     2,    10, ..., 57455, 57459, 57461])),\n",
              " (array([    0,     1,     2, ..., 57468, 57469, 57470]),\n",
              "  array([    7,     8,    13, ..., 57474, 57475, 57476])),\n",
              " (array([    1,     2,     3, ..., 57474, 57475, 57476]),\n",
              "  array([    0,     5,     9, ..., 57457, 57463, 57469]))]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"number of splits {len(splits)}, each splits contain {len(splits[0])}, and within it {len(splits[0][0])}: {len(splits[0][1])}, ideally {len(train)//5}\")"
      ],
      "metadata": {
        "_uuid": "3480d341-66d2-4261-ac4a-b9eb7c0c0361",
        "_cell_guid": "9b3e1a6b-3a6c-4c4c-9d73-491021e0001a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:00.109407Z",
          "iopub.execute_input": "2025-02-23T00:03:00.109741Z",
          "iopub.status.idle": "2025-02-23T00:03:00.126773Z",
          "shell.execute_reply.started": "2025-02-23T00:03:00.109714Z",
          "shell.execute_reply": "2025-02-23T00:03:00.125484Z"
        },
        "id": "Nu-cdUo9y665",
        "outputId": "64903d14-b5ff-400f-d1c2-3e3d9ccecb85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of splits 5, each splits contain 2, and within it 46031: 11446, ideally 11495\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "splits[0][1]"
      ],
      "metadata": {
        "_uuid": "8fae4cc8-7994-43c4-b56b-e3850eafae42",
        "_cell_guid": "7db22209-7493-4497-be60-73c76d56d4f4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:00.127687Z",
          "iopub.execute_input": "2025-02-23T00:03:00.128001Z",
          "iopub.status.idle": "2025-02-23T00:03:00.146952Z",
          "shell.execute_reply.started": "2025-02-23T00:03:00.127974Z",
          "shell.execute_reply": "2025-02-23T00:03:00.145955Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "5zbeuQLRy665",
        "outputId": "d567d926-99d3-4cb9-f367-86860b3e1fad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    3,    21,    33, ..., 57467, 57468, 57470])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "group_id"
      ],
      "metadata": {
        "_uuid": "219fd03b-dcb7-4cfd-b11c-ebdfe207586a",
        "_cell_guid": "e4720180-2c29-4d9d-98b2-c6888f832925",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:00.147971Z",
          "iopub.execute_input": "2025-02-23T00:03:00.148604Z",
          "iopub.status.idle": "2025-02-23T00:03:00.163894Z",
          "shell.execute_reply.started": "2025-02-23T00:03:00.148567Z",
          "shell.execute_reply": "2025-02-23T00:03:00.162892Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "B9Alc6mfy665",
        "outputId": "459c5036-05e3-4c23-b228-debefcf2f8fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [\"Is it morally right to try to have a certain...\n",
              "1        [\"What is the difference between marriage lice...\n",
              "2        [\"explain function calling. how would you call...\n",
              "3        [\"How can I create a test set for a very rare ...\n",
              "4        [\"What is the best way to travel from Tel-Aviv...\n",
              "                               ...                        \n",
              "57472    [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...\n",
              "57473    [\"In python, implement a naive Bayes with gaus...\n",
              "57474    [\"is it unethical to work on building weapons?...\n",
              "57475    [\"If a bait contains 0,0025% bromadiolon then ...\n",
              "57476    [\"three kids eat three apples in three days, h...\n",
              "Name: prompt, Length: 57477, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[\"Is it morally right to try to have a certain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[\"What is the difference between marriage lice...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[\"explain function calling. how would you call...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[\"How can I create a test set for a very rare ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57472</th>\n",
              "      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57473</th>\n",
              "      <td>[\"In python, implement a naive Bayes with gaus...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57474</th>\n",
              "      <td>[\"is it unethical to work on building weapons?...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57475</th>\n",
              "      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57476</th>\n",
              "      <td>[\"three kids eat three apples in three days, h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>57477 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "label_id"
      ],
      "metadata": {
        "_uuid": "93540edd-bacf-467d-9ee6-49eaaf73ca23",
        "_cell_guid": "e750255e-c700-425b-9a32-e21d80df724e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:00.164966Z",
          "iopub.execute_input": "2025-02-23T00:03:00.165385Z",
          "iopub.status.idle": "2025-02-23T00:03:00.182038Z",
          "shell.execute_reply.started": "2025-02-23T00:03:00.16535Z",
          "shell.execute_reply": "2025-02-23T00:03:00.181107Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "R5L0MDmoy666",
        "outputId": "0f2f0f2b-8ac6-457e-96f2-0dabd877133d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, ..., 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(group_id.unique()))"
      ],
      "metadata": {
        "_uuid": "bc87bf2f-311b-498c-bf37-bb5e68db73b7",
        "_cell_guid": "7dc9fb10-0d28-479a-a6a1-fcd2b75567e8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:00.186209Z",
          "iopub.execute_input": "2025-02-23T00:03:00.186541Z",
          "iopub.status.idle": "2025-02-23T00:03:00.259333Z",
          "shell.execute_reply.started": "2025-02-23T00:03:00.186511Z",
          "shell.execute_reply": "2025-02-23T00:03:00.258216Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "GfU82wRay666",
        "outputId": "fcb88b06-59c5-47af-e4f5-a987f53b8022",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51734\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train[\"fold\"] = -1\n",
        "for fold, (_, valid_idx) in enumerate(splits):\n",
        "    train.loc[valid_idx, \"fold\"] = fold\n",
        "\n",
        "train.head(1)"
      ],
      "metadata": {
        "_uuid": "581d53a6-5a5d-423d-a1a5-b95afdbe4ca6",
        "_cell_guid": "6dcb9608-b3ee-4c76-9562-c4a93ccf525a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:00.261362Z",
          "iopub.execute_input": "2025-02-23T00:03:00.261644Z",
          "iopub.status.idle": "2025-02-23T00:03:00.287351Z",
          "shell.execute_reply.started": "2025-02-23T00:03:00.26162Z",
          "shell.execute_reply": "2025-02-23T00:03:00.286404Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "lNK01M5By666",
        "outputId": "3756d9d9-eaf0-4a14-ea20-aab9c1d19daa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id             model_a     model_b  \\\n",
              "0  30192  gpt-4-1106-preview  gpt-4-0613   \n",
              "\n",
              "                                              prompt  \\\n",
              "0  [\"Is it morally right to try to have a certain...   \n",
              "\n",
              "                                          response_a  \\\n",
              "0  [\"The question of whether it is morally right ...   \n",
              "\n",
              "                                          response_b  winner_model_a  \\\n",
              "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
              "\n",
              "   winner_model_b  winner_tie  fold  \n",
              "0               0           0     4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62dfad01-9f96-4c15-ac7f-e35ba7213610\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>model_a</th>\n",
              "      <th>model_b</th>\n",
              "      <th>prompt</th>\n",
              "      <th>response_a</th>\n",
              "      <th>response_b</th>\n",
              "      <th>winner_model_a</th>\n",
              "      <th>winner_model_b</th>\n",
              "      <th>winner_tie</th>\n",
              "      <th>fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30192</td>\n",
              "      <td>gpt-4-1106-preview</td>\n",
              "      <td>gpt-4-0613</td>\n",
              "      <td>[\"Is it morally right to try to have a certain...</td>\n",
              "      <td>[\"The question of whether it is morally right ...</td>\n",
              "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62dfad01-9f96-4c15-ac7f-e35ba7213610')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-62dfad01-9f96-4c15-ac7f-e35ba7213610 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-62dfad01-9f96-4c15-ac7f-e35ba7213610');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train",
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 57477,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1238326827,\n        \"min\": 30192,\n        \"max\": 4294947231,\n        \"num_unique_values\": 57477,\n        \"samples\": [\n          2785062085,\n          48259531,\n          3622731894\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_a\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          \"mistral-7b-instruct\",\n          \"guanaco-33b\",\n          \"gpt-4-1106-preview\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_b\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          \"llama2-70b-steerlm-chat\",\n          \"llama-13b\",\n          \"gpt-4-0613\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51734,\n        \"samples\": [\n          \"[\\\"Are autoregressive language models (e.g. GPT-3) currently more popular than auto-encoding language models (e.g. BERT)? If so, what are the explanations?\\\"]\",\n          \"[\\\"Write a c++ algorithm that aims to find the rightmost zero in a binary representation, flip it to a one, and flip any 1s to its right\\\"]\",\n          \"[\\\"What is J\\\\u00e9r\\\\u00f4me WAX ?\\\"]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response_a\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 56566,\n        \"samples\": [\n          \"[\\\"Tea is grown commercially in the Azores, an autonomous region of Portugal. The main tea-producing estates are located on the island of S\\\\u00e3o Miguel, where the only commercial tea plantations in Europe can be found. The most well-known tea estates on S\\\\u00e3o Miguel are Gorreana Tea Estate and Porto Formoso Tea Factory.\\\"]\",\n          \"[\\\"Sure! Here's an example to illustrate how function pointers work in C:\\\\n\\\\n```c\\\\n#include <stdio.h>\\\\n\\\\n\\\\/\\\\/ Function to be pointed to\\\\nint add(int a, int b) {\\\\n    return a + b;\\\\n}\\\\n\\\\n\\\\/\\\\/ Function to demonstrate function pointers\\\\nvoid functionPointerExample(int (*funcPtr)(int, int)) {\\\\n    int result = funcPtr(5, 10);\\\\n    printf(\\\\\\\"Result: %d\\\\\\\\n\\\\\\\", result);\\\\n}\\\\n\\\\nint main() {\\\\n    \\\\/\\\\/ Use the address-of operator (&) to get the address of the add function\\\\n    int (*addPtr)(int, int) = &add;\\\\n\\\\n    \\\\/\\\\/ Call the functionPointerExample function and pass the address of the add function\\\\n    functionPointerExample(addPtr);\\\\n\\\\n    \\\\/\\\\/ Call the add function directly\\\\n    int result = add(15, 20);\\\\n    printf(\\\\\\\"Result: %d\\\\\\\\n\\\\\\\", result);\\\\n\\\\n    return 0;\\\\n}\\\\n```\\\\n\\\\nIn this example, we define a function `add` that takes two integers as arguments and returns their sum.\\\\n\\\\nWe then define a function `functionPointerExample` that takes a function pointer as an argument. This function calls the pointed-to function passing it two integers, and then prints the result.\\\\n\\\\nIn the main function, we create a function pointer `addPtr` which points to the `add` function. We then call the `functionPointerExample` function passing the address of the `add` function using the `&` operator.\\\\n\\\\nFinally, we call the `add` function directly and store the result in a variable `result`.\\\\n\\\\nI hope this helps you understand how function pointers work in C! Let me know if you have any other questions.\\\",\\\"To define a function pointer within a struct, you can define a member variable of the struct as a pointer to a function with a specific return type and argument list. Here's an example:\\\\n\\\\n```c\\\\n#include <stdio.h>\\\\n\\\\n\\\\/\\\\/ Define a structure with a function pointer member\\\\ntypedef struct {\\\\n    int (*calculate)(int);\\\\n} Calculator;\\\\n\\\\n\\\\/\\\\/ Function to be pointed to\\\\nint add(int num) {\\\\n    return num + 10;\\\\n}\\\\n\\\\n\\\\/\\\\/ Function to demonstrate using a function pointer within a struct\\\\nvoid useCalculator(Calculator calculator) {\\\\n    int result = calculator.calculate(15);\\\\n    printf(\\\\\\\"Result: %d\\\\\\\\n\\\\\\\", result);\\\\n}\\\\n\\\\nint main() {\\\\n    \\\\/\\\\/ Create a Calculator struct and initialize the function pointer member\\\\n    Calculator calculator = { add };\\\\n\\\\n    \\\\/\\\\/ Call the useCalculator function and pass the calculator struct\\\\n    useCalculator(calculator);\\\\n\\\\n    return 0;\\\\n}\\\\n```\\\\n\\\\nIn this example, we define a `typedef` for our `Calculator` struct, which has a single member `calculate` that is a pointer to a function that takes an integer as an argument and returns an integer.\\\\n\\\\nWe then define a function `add` that adds 10 to the input integer and return the result.\\\\n\\\\nWe define a function `useCalculator` that takes a `Calculator` struct as an argument and invokes the `calculate` member function of the struct with an argument of 15.\\\\n\\\\nIn the `main` function, we create a `Calculator` struct and initialize its `calculate` member with the address of the `add` function. We then call the `useCalculator` function passing the `calculator` struct.\\\\n\\\\nI hope this helps you understand how to define a function pointer within a struct in C! Let me know if you have any other questions.\\\"]\",\n          \"[\\\"In the depths of a dark and mysterious cave, there lived a heavily pregnant cavewoman named Ula. Her thick, matted hair cascaded down her back, and her skin was tanned and rough from years of living in the harsh wilderness. Ula's husband, Kadan, was a skilled hunter and provided for their small tribe. But lately, even his sharpest arrows seemed to fall short, and food had become scarce.\\\\n\\\\nUla felt a deep ache in her bones and her swollen belly. It was the third trimester, and her baby was growing rapidly. She wondered if she would ever be able to give birth in the cramped, narrow cave. Ula's tribe had no midwife, and her only hope was to rely on the wisdom passed down from her own mother.\\\\n\\\\nOne day, while Kadan was out hunting, Ula went into labor. Her screams echoed through the cave, and her tribe gathered around her in a frenzy. Ula's eyes widened as she felt the intense pain of contractions. She was afraid for her baby's life and her own.\\\\n\\\\nThe tribe tried to comfort Ula, but their efforts fell short. Ula's baby was too big, and the birth seemed impossible. Hours passed, and Ula lost all strength. Her tribe watched in horror as Ula's life slipped away. Suddenly, a loud cry broke the silence. Ula's baby had been born, and he was healthy and strong.\\\\n\\\\nThe tribe rejoiced at the sight of the newborn. They named him Kade, after Ula's husband, Kadan. Kade was a symbol of hope and resilience, and his birth brought new life and energy to the tribe.\\\\n\\\\nAs Ula took her last breath, she whispered to Kade, \\\\\\\"May you grow up to be a fierce and proud caveman, just like your father.\\\\\\\" Ula's legacy lived on through her son, and her tribe honored her memory by carving her name into the walls of the cave. The story of Ula, the heavily pregnant cavewoman who defied death to give birth to Kade, became a legend that would be passed down through the generations.\\\"]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response_b\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 56609,\n        \"samples\": [\n          \"[\\\"Unfortunately, I do not have enough context to provide specific details about \\\\\\\"mobility123\\\\\\\" in New Jersey. Mobility123 is not a term I am familiar with. To provide more useful information, it would be helpful if you could clarify what kind of service, company, or product mobility123 refers to, as well as any additional details about what you would like to know in relation to New Jersey. Please provide some more context and details so that I can try to assist. Let me know if you can share more information!\\\"]\",\n          \"[\\\"Abby, Amber, Amra, Arin, Arna, Ashy, Bari, Bett, Brix, Bryn, Brin, Brynne, Ciri, Cyrn, Ebby, Ebri, Elbr, Elmry, Embr, Raby, Reba\\\"]\",\n          \"[\\\"It's great that your child is considering how to communicate with her friends in a way that doesn't upset them. Here are some tips that may help:\\\\n\\\\n1. Listen actively: Encourage your child to listen to her friends without interrupting or judging them. This means paying attention to what they're saying, asking questions to clarify their thoughts, and acknowledging their feelings.\\\\n2. Use kind words: Remind your child to use kind and respectful language when talking to her friends. This means avoiding hurtful words or tone, and being mindful of how her words may be perceived by others.\\\\n3. Be empathetic: Encourage your child to try to see things from her friends' perspective and to understand their feelings and needs. This means putting herself in their shoes and considering how they might be feeling.\\\\n4. Avoid gossip: Remind your child that gossiping about her friends can be hurtful and damaging to their relationships. Encourage her to avoid talking about her friends behind their backs and to speak to them directly if she has concerns.\\\\n5. Be honest: Remind your child that honesty is always the best policy, even if it's difficult to hear. Encourage her to be open and truthful with her friends, but also to be gentle and considerate in how she delivers her message.\\\\n\\\\nBy following these tips, your child can learn how to communicate effectively with her friends without upsetting them.\\\"]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"winner_model_a\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"winner_model_b\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"winner_tie\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train['fold'].value_counts()"
      ],
      "metadata": {
        "_uuid": "7b91ba66-968c-4ad2-80d2-ffc77fa87e06",
        "_cell_guid": "b21072a7-e5b4-49e7-a945-1a9e3354a3af",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:00.28831Z",
          "iopub.execute_input": "2025-02-23T00:03:00.288583Z",
          "iopub.status.idle": "2025-02-23T00:03:00.297521Z",
          "shell.execute_reply.started": "2025-02-23T00:03:00.288559Z",
          "shell.execute_reply": "2025-02-23T00:03:00.296577Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "33x_nTuVy666"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "\n",
        "train_original = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
        "external_data = load_dataset(\"lmsys/chatbot_arena_conversations\")[\"train\"].to_pandas()\n",
        "\n",
        "print('-----------external_data-----------')\n",
        "print(external_data.head(1))\n",
        "print(external_data.shape)\n",
        "print(external_data.columns)"
      ],
      "metadata": {
        "_uuid": "6f316733-f26b-4bbc-969e-a9e1f1387931",
        "_cell_guid": "b9ff34d0-fe41-4c86-8ab0-63881b7b57fc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:00.298625Z",
          "iopub.execute_input": "2025-02-23T00:03:00.298992Z",
          "iopub.status.idle": "2025-02-23T00:03:09.061981Z",
          "shell.execute_reply.started": "2025-02-23T00:03:00.298958Z",
          "shell.execute_reply": "2025-02-23T00:03:09.061021Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "gF1McEPGy666"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_conv(conv):\n",
        "    try:\n",
        "        user_texts = [x[\"content\"] for x in conv if x[\"role\"] == \"user\"]\n",
        "        assistant_texts = [x[\"content\"] for x in conv if x[\"role\"] == \"assistant\"]\n",
        "\n",
        "        return user_texts, json.dumps(assistant_texts)\n",
        "    except:\n",
        "        print(conv)\n",
        "\n",
        "external_data[\"prompt_a\"], external_data[\"response_a\"] = zip(*external_data.conversation_a.progress_apply(separate_conv))\n",
        "external_data[\"prompt_b\"], external_data[\"response_b\"] = zip(*external_data.conversation_b.progress_apply(separate_conv))\n",
        "external_data[\"prompt\"] = external_data[\"prompt_a\"].progress_apply(json.dumps)\n",
        "print(external_data.winner.value_counts())\n",
        "\n",
        "def one_hot_encode(winner):\n",
        "    return pd.Series([int(\"model_a\" == winner), int(\"model_b\" == winner), int(\"tie\" == winner or \"tie (bothbad)\" == winner)])\n",
        "\n",
        "external_data[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] = (external_data.winner.progress_apply(one_hot_encode))\n",
        "cols = [\"question_id\",\"model_a\",\"model_b\",\"prompt\",\"response_a\",\"response_b\",\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]\n",
        "external_data = pd.DataFrame(external_data[cols].copy().values, columns=train_original.columns)\n",
        "\n",
        "superset = pd.concat([external_data, train_original]).reset_index(drop=True)\n",
        "external_data_deduplicated = superset.drop_duplicates(subset=[\"prompt\"], keep=\"last\")\n",
        "external_data_deduplicated = external_data_deduplicated[external_data_deduplicated.index.isin(external_data.index)]\n",
        "print(len(external_data_deduplicated))\n",
        "print(external_data_deduplicated.head(1))\n",
        "\n",
        "# external_data.to_csv(\"output/lmsys-33k.csv\", index=False)\n",
        "external_data_deduplicated = external_data_deduplicated.reset_index(drop=True)\n",
        "# external_data_deduplicated.to_csv(\"output/lmsys-33k-deduplicated.csv\", index=False)"
      ],
      "metadata": {
        "_uuid": "f255872e-07fc-4fdf-b67e-f8150cb870f5",
        "_cell_guid": "6a397ca3-11b1-4efd-8607-442d25b3fa34",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:09.062908Z",
          "iopub.execute_input": "2025-02-23T00:03:09.063281Z",
          "iopub.status.idle": "2025-02-23T00:03:14.584572Z",
          "shell.execute_reply.started": "2025-02-23T00:03:09.063246Z",
          "shell.execute_reply": "2025-02-23T00:03:14.583503Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "WNvKxQCcy666"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----------external_data_deduplicated-----------')\n",
        "print(external_data_deduplicated.head(1))\n",
        "print(external_data_deduplicated.shape)\n",
        "print(external_data_deduplicated.columns)\n",
        "\n",
        "print('-----------train-----------')\n",
        "print(train.head(1))\n",
        "print(train.shape)\n",
        "print(train.columns)"
      ],
      "metadata": {
        "_uuid": "1a2c04f0-b3b6-42c4-9c04-b0bad9f0d44c",
        "_cell_guid": "16983cf1-42a1-45bb-83a7-629e93009d05",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:14.585461Z",
          "iopub.execute_input": "2025-02-23T00:03:14.585774Z",
          "iopub.status.idle": "2025-02-23T00:03:14.598701Z",
          "shell.execute_reply.started": "2025-02-23T00:03:14.585749Z",
          "shell.execute_reply": "2025-02-23T00:03:14.597744Z"
        },
        "id": "7Ej0FlFJy667"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Config file**\n",
        "- dataset building"
      ],
      "metadata": {
        "_uuid": "27027d0f-d7a1-4cd2-b073-36a914264f4e",
        "_cell_guid": "eaea0953-1720-4100-b266-0aa454697734",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "6nyMidiuy667"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer**\n",
        "\n",
        "```bash\n",
        "GemmaTokenizerFast(\n",
        "        name_or_path='google/gemma-2-9b-it',\n",
        "        vocab_size=256000,\n",
        "        model_max_length=1000000000000000019884624838656,\n",
        "        is_fast=True,\n",
        "        padding_side='left',\n",
        "        truncation_side='right',\n",
        "        special_tokens={'bos_token': '<bos>',\n",
        "                        'eos_token': '<eos>',\n",
        "                        'unk_token': '<unk>',\n",
        "                        'pad_token': '<pad>',\n",
        "                        'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']\n",
        "                        },\n",
        "        clean_up_tokenization_spaces=False,\n",
        "        added_tokens_decoder = {\n",
        "                        \t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
        "                        \t1: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
        "                        \t2: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
        "                            ...\n",
        "                            ...\n",
        "                            ...\n",
        "                            ...\n",
        "                            ...\n",
        "                            255998: AddedToken(\"\t\t\t   \t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
        "                        \t255999: AddedToken(\"<unused99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
        "                            }\n",
        "                )\n",
        "```\n",
        "\n",
        "- SentencePiece Tokenizer: Used for subword tokenization and byte-level encoding.\n",
        "- Special Tokens: Added to structure the input and output sequences.\n",
        "- Tokenization Steps: Subword splitting, byte-level encoding, and adding special tokens.\n",
        "- Output: A list of token IDs suitable for input into the model.\n",
        "- This tokenization process ensures that the model can handle a wide range of text inputs efficiently and effectively, making it suitable for various natural language processing task.\n",
        "\n",
        "\n",
        "1. **SentencePiece Tokenizer**\n",
        "\n",
        "Gemma models use the SentencePiece tokenizer, which is a subword tokenizer. This tokenizer splits text into subwords, which helps in handling out-of-vocabulary (OOV) words and improves the efficiency of the model.\n",
        "Key Features:\n",
        "- Byte-Level Encoding: SentencePiece uses byte-level encoding, which means it can handle any Unicode text.\n",
        "- Subword Tokenization: It splits words into subwords, which helps in managing rare words and unseen words during training.\n",
        "- Preserved Whitespace: It preserves whitespace, which is important for maintaining the structure of the text.\n",
        "- Trained on multilingual data.\n",
        "- Uses ▁ to denote word boundaries.\n",
        "\n",
        "2. **Tokenization Process**\n",
        "Step-by-Step Tokenization:\n",
        "- Text Input: The raw text is provided to the tokenizer.\n",
        "- Subword Splitting: The SentencePiece tokenizer splits the text into subwords.\n",
        "- Byte-Level Encoding: Each subword is encoded into bytes.\n",
        "- Special Tokens: Special tokens like <bos>, <eos>, <pad>, and others are added to the tokenized text to mark the beginning and end of sequences and to handle padding.\n",
        "\n",
        "3. **Special Tokens**\n",
        "\n",
        "Gemma models use several special tokens to structure the input and output sequences:\n",
        "- `<bos>`: Beginning of Sequence token.\n",
        "- `<eos>`: End of Sequence token.\n",
        "- `<pad>`: Padding token.\n",
        "- `<start_of_turn>`: Indicates the start of a turn in a multi-turn conversation.\n",
        "- `<end_of_turn>`: Indicates the end of a turn in a multi-turn conversation.\n",
        "\n",
        "These special tokens help the model understand the structure of the input and generate appropriate responses.\n",
        "\n",
        "\n",
        "3. Key Differences\n",
        "\n",
        "| Feature           | BERT       | Gemma 2    | Llama 3    |\n",
        "|-------------------|------------|------------|------------|\n",
        "| Algorithm         | WordPiece  | SentencePiece | BPE        |\n",
        "| Vocabulary Size   | ~30k       | ~256k      | 128k       |\n",
        "| Special Tokens    | [CLS], [SEP] | `<bos>`, `<eos>` | `<s>, </s>`  |\n",
        "| Case Handling     | Optional   | Case-sensitive | Case-sensitive |\n",
        "| Whitespace        | Ignored    | ▁ markers  | Explicit spaces |\n",
        "| Emoji Handling    | Poor       | Good       | Excellent |\n",
        "| Multilingual      | No         | Yes        | Yes        |\n",
        "\n",
        "\n",
        "**pretrained_model_functionality from transformer**\n",
        "- https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/model#transformers.PreTrainedModel\n",
        "- about gemma2: https://huggingface.co/docs/transformers/en/model_doc/gemma2"
      ],
      "metadata": {
        "_uuid": "761987ce-de87-48fa-b5da-b9c86513d667",
        "_cell_guid": "2f2ff27b-3771-4832-885f-1e4b74ae1aa3",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "wGk85kgxy667"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing of tokenizer process\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer_testing = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
        "\n",
        "# Example text\n",
        "text_testing = \"This is an example sentence model path when have acces to dowload weights from HF\"\n",
        "max_length1 = 10\n",
        "# tokenizer(text, add_special_tokens=False, max_length=max_length, truncation=True).input_ids\n",
        "\n",
        "# Tokenize the text\n",
        "inputs_testing = tokenizer_testing(text_testing, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
        "inputs_testing1 = tokenizer_testing(text_testing, add_special_tokens=False, max_length=max_length1, truncation=True)\n",
        "\n",
        "# Print the tokenized input\n",
        "print(inputs_testing)\n",
        "print(inputs_testing1)\n",
        "print(len(text_testing))\n",
        "print(len(inputs_testing1.input_ids))\n",
        "# print(len(inputs_testing['input_ids'][0]))\n",
        "# print(len(inputs_testing['attention_mask']))"
      ],
      "metadata": {
        "_uuid": "9c082ac2-cabb-4efc-ae82-789d2a5d9649",
        "_cell_guid": "238a905c-ad5b-45c9-a669-c5569be528e3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:14.599512Z",
          "iopub.execute_input": "2025-02-23T00:03:14.599823Z",
          "iopub.status.idle": "2025-02-23T00:03:27.201325Z",
          "shell.execute_reply.started": "2025-02-23T00:03:14.599798Z",
          "shell.execute_reply": "2025-02-23T00:03:27.200216Z"
        },
        "id": "zPjlmH1ay668"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name_or_path = \"google/gemma-2-9b-it\" # model path when have acces to dowload weights from HF\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "tokenizer1"
      ],
      "metadata": {
        "_uuid": "398de8d5-8fbd-41d7-931e-e972bafa6f32",
        "_cell_guid": "a521ae6b-1bf1-4665-96d7-d9aa97c9d19e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:27.202459Z",
          "iopub.execute_input": "2025-02-23T00:03:27.202941Z",
          "iopub.status.idle": "2025-02-23T00:03:28.991692Z",
          "shell.execute_reply.started": "2025-02-23T00:03:27.202912Z",
          "shell.execute_reply": "2025-02-23T00:03:28.990633Z"
        },
        "_kg_hide-output": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "sDyJs8gfy668"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Preparation**\n",
        "\n",
        "Using Map style dataset for this:\n",
        "\n",
        "![image.png](attachment:cce76ede-fdfb-4702-9764-76ed3efec276.png)\n",
        "\n",
        "Other type is iterable."
      ],
      "metadata": {
        "_uuid": "342932f1-a9e6-4f5b-ab2a-ba406eee9e2d",
        "_cell_guid": "a8227467-f4b3-4c76-95ca-1f23b5f176bc",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Hg39mdRey668"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 1\n",
        "data1 = train\n",
        "\n",
        "data = data1.iloc[idx].to_dict()\n",
        "\n",
        "print(f\"data: {data}\\n\\n\")\n",
        "\n",
        "prompts = json.loads(data[\"prompt\"])\n",
        "responses_a = json.loads(data[\"response_a\"])\n",
        "responses_b = json.loads(data[\"response_b\"]),"
      ],
      "metadata": {
        "_uuid": "dca3962f-62f3-4116-864f-5a6f2dbe3f3e",
        "_cell_guid": "b555e786-358a-4726-bbb9-dda963dad1d0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:28.992668Z",
          "iopub.execute_input": "2025-02-23T00:03:28.992942Z",
          "iopub.status.idle": "2025-02-23T00:03:28.999534Z",
          "shell.execute_reply.started": "2025-02-23T00:03:28.992919Z",
          "shell.execute_reply": "2025-02-23T00:03:28.998308Z"
        },
        "id": "OKa4vrtYy668"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "max_length1 = 4096\n",
        "support_system_role1 = False\n",
        "\n",
        "tokenizer = tokenizer1\n",
        "max_length = max_length1\n",
        "support_system_role = support_system_role1\n",
        "\n",
        "PROMPT_PREFIX = \"\"\"Please act as an impartial judge and evaluate the quality of the responses provided by two\n",
        "AI assistants to the user question displayed below. You should choose the assistant that\n",
        "follows the user’s instructions and answers the user’s question better. Your evaluation\n",
        "should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\n",
        "and level of detail of their responses. Begin your evaluation by comparing the two\n",
        "responses and provide a short explanation. Avoid any position biases and ensure that the\n",
        "order in which the responses were presented does not influence your decision. Do not allow\n",
        "the length of the responses to influence your evaluation. Do not favor certain names of\n",
        "the assistants. Be as objective as possible. After providing your explanation, output your\n",
        "final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n",
        "if assistant B is better, and \"[[C]]\" for a tie.\"\"\"\n",
        "\n",
        "PROMPT_SUFFIX = \"verdict is: [[\"\n",
        "\n",
        "LABEL_COLS = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
        "\n",
        "head = \"<|The Start of Conversation between a User and two Assistants|>\"\n",
        "tail = \"<|The End of Conversation between a User and two Assistants|>\\n\"\n",
        "parts = []\n",
        "\n",
        "for prompt, response_a, response_b in zip(prompts, responses_a, responses_b):\n",
        "    if prompt is None:\n",
        "        prompt = \"null\"\n",
        "    if response_a is None:\n",
        "        response_a = \"null\"\n",
        "    if response_b is None:\n",
        "        response_b = \"null\"\n",
        "    parts.append(f\"\\n### User:\\n{prompt}\\n\\n### Assistant A:\\n{response_a}\\n\\n### Assistant B:\\n{response_b}\\n\")\n",
        "\n",
        "text = \"\".join(parts)\n",
        "print(text)\n",
        "print(tokenizer(text, add_special_tokens=False, max_length=max_length, truncation=True))\n",
        "input_ids = tokenizer(text, add_special_tokens=False, max_length=max_length, truncation=True).input_ids\n",
        "truncated_text = tokenizer.decode(input_ids)\n",
        "# print(truncated_text)\n",
        "\n",
        "conversation = head + truncated_text + tail\n",
        "print(f\"input_ids length with truncation: {len(input_ids)}\\n\\n\")\n",
        "print(f\"truncated_text length: {len(truncated_text.split())}\\n\\n\")\n",
        "print(f\"conversation: {conversation}\\n\\n\")\n",
        "print(f\"conversation words length: {len(conversation.split())}\\n\\n\")\n",
        "\n",
        "if support_system_role:\n",
        "    messages = [{\"role\": \"system\", \"content\": PROMPT_PREFIX}, {\"role\": \"user\", \"content\": conversation}]\n",
        "else:\n",
        "    messages = [{\"role\": \"user\", \"content\": f\"{PROMPT_PREFIX}\\n{conversation}\"}]\n",
        "\n",
        "input_text = (tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) + PROMPT_SUFFIX)\n",
        "input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\").input_ids[0]\n",
        "label = torch.tensor([data[col] for col in LABEL_COLS]).float()\n",
        "\n",
        "print(f\"after apply_chat_template input_ids length: {len(input_ids)}\\n\\n\")\n",
        "print(f\"after apply_chat_template input_text words length: {len(input_text.split())}\\n\\n\")\n",
        "print(f\"label length: {len(label)}\\n\\n\")\n",
        "\n",
        "# import numpy as np\n",
        "# np.set_printoptions(threshold=np.inf)\n",
        "# print(input_ids.numpy())\n",
        "\n",
        "processorPAB = dict(input_ids=input_ids, input_text=input_text, label=label)\n",
        "\n",
        "print(f\"\\n\\nprocessorPAB: {processorPAB}\")"
      ],
      "metadata": {
        "_uuid": "d724dd80-23cd-4db5-88fb-9d1fb121ce15",
        "_cell_guid": "fdebfbfa-036b-4a48-882a-1cb644a07f8b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:29.000735Z",
          "iopub.execute_input": "2025-02-23T00:03:29.001103Z",
          "iopub.status.idle": "2025-02-23T00:03:42.925051Z",
          "shell.execute_reply.started": "2025-02-23T00:03:29.001047Z",
          "shell.execute_reply": "2025-02-23T00:03:42.923881Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "nyhtBdZwy669"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "max_length1 = 4096 // 2\n",
        "support_system_role1 = False\n",
        "\n",
        "tokenizer = tokenizer1\n",
        "max_length = max_length1\n",
        "support_system_role = support_system_role1\n",
        "\n",
        "head = \"<|The Start of Assistant A’s Conversation with User|>\"\n",
        "sep = \"<|The End of Assistant A’s Conversation with User|>\\n\\n<|The Start of Assistant B’s Conversation with User|>\"\n",
        "tail = \"<|The End of Assistant B’s Conversation with User|>\\n\"\n",
        "parts_a = []\n",
        "parts_b = []\n",
        "\n",
        "for prompt, response_a, response_b in zip(prompts, responses_a, responses_b):\n",
        "    if prompt is None:\n",
        "        prompt = \"null\"\n",
        "    if response_a is None:\n",
        "        response_a = \"null\"\n",
        "    if response_b is None:\n",
        "        response_b = \"null\"\n",
        "    parts_a.append(f\"\\n### User:\\n{prompt}\\n\\n### Assistant A:\\n{response_a}\\n\")\n",
        "    parts_b.append(f\"\\n### User:\\n{prompt}\\n\\n### Assistant B:\\n{response_b}\\n\")\n",
        "\n",
        "text_a = \"\".join(parts_a)\n",
        "text_b = \"\".join(parts_b)\n",
        "\n",
        "input_ids_a = tokenizer(text_a, add_special_tokens=False, max_length=max_length, truncation=True).input_ids\n",
        "input_ids_b = tokenizer(text_b, add_special_tokens=False, max_length=max_length, truncation=True).input_ids\n",
        "\n",
        "truncated_text_a = tokenizer.decode(input_ids_a)\n",
        "truncated_text_b = tokenizer.decode(input_ids_b)\n",
        "\n",
        "conversation = head + truncated_text_a + sep + truncated_text_b + tail\n",
        "print(f\"conversation: {conversation}\\n\\n\")\n",
        "\n",
        "if support_system_role:\n",
        "    messages = [{\"role\": \"system\", \"content\": PROMPT_PREFIX}, {\"role\": \"user\", \"content\": conversation}]\n",
        "else:\n",
        "    messages = [{\"role\": \"user\", \"content\": f\"{PROMPT_PREFIX}\\n{conversation}\"}]\n",
        "\n",
        "input_text = (tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) + PROMPT_SUFFIX)\n",
        "input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\").input_ids[0]\n",
        "label = torch.tensor([data[col] for col in LABEL_COLS]).float()\n",
        "\n",
        "ProcessorPAPB = dict(input_ids=input_ids, input_text=input_text, label=label)\n",
        "\n",
        "print(f\"ProcessorPAPB: {ProcessorPAPB}\")"
      ],
      "metadata": {
        "_uuid": "a6d398a1-782d-4726-ac5c-cb745cf5451b",
        "_cell_guid": "462f4b38-f38e-4dfe-8036-280139918445",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:42.926342Z",
          "iopub.execute_input": "2025-02-23T00:03:42.926954Z",
          "iopub.status.idle": "2025-02-23T00:03:42.960919Z",
          "shell.execute_reply.started": "2025-02-23T00:03:42.926923Z",
          "shell.execute_reply": "2025-02-23T00:03:42.959781Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "jGK8EElGy669"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# processor = [ProcessorPAB, ProcessorPAPB]\n",
        "\n",
        "import pandas as pd\n",
        "import copy\n",
        "\n",
        "# Define the processors\n",
        "class ProcessorPAB:\n",
        "    def __init__(self, tokenizer, max_length, support_system_role):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.support_system_role = support_system_role\n",
        "\n",
        "    def build_input(self, data):\n",
        "        # Example processing (you need to implement the actual processing logic)\n",
        "        return {\n",
        "            'input_ids': self.tokenizer.encode(data['prompt'], max_length=self.max_length, truncation=True),\n",
        "            'labels': data['winner_model_a']\n",
        "        }\n",
        "\n",
        "class ProcessorPAPB:\n",
        "    def __init__(self, tokenizer, max_length, support_system_role):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.support_system_role = support_system_role\n",
        "\n",
        "    def build_input(self, data):\n",
        "        # Example processing (you need to implement the actual processing logic)\n",
        "        return {\n",
        "            'input_ids': self.tokenizer.encode(data['prompt'], max_length=self.max_length, truncation=True),\n",
        "            'labels': data['winner_model_b']\n",
        "        }\n",
        "\n",
        "# Initialize processors\n",
        "processor = [\n",
        "    ProcessorPAB(tokenizer=tokenizer, max_length=max_length, support_system_role=False),\n",
        "    ProcessorPAPB(tokenizer=tokenizer, max_length=max_length // 2, support_system_role=False)\n",
        "]\n",
        "\n",
        "# Load the DataFrame\n",
        "# df = pd.read_csv(\"../output/dtrainval.csv\")\n",
        "df = train\n",
        "include_swap = True\n",
        "\n",
        "# Apply the query\n",
        "query = f\"fold != {fold}\"\n",
        "df = df.query(query).reset_index(drop=True)\n",
        "\n",
        "# Process each sample\n",
        "dataset0 = []\n",
        "for idx in range(len(df[0:20])):\n",
        "    data = df.iloc[idx].to_dict()\n",
        "    ret = [proc.build_input(data) for proc in processor]\n",
        "\n",
        "    if include_swap:\n",
        "        data_swap = copy.deepcopy(data)\n",
        "        data_swap[\"model_a\"], data_swap[\"model_b\"] = data_swap[\"model_b\"], data_swap[\"model_a\"]\n",
        "        data_swap[\"response_a\"], data_swap[\"response_b\"] = data_swap[\"response_b\"], data_swap[\"response_a\"]\n",
        "        data_swap[\"winner_model_a\"], data_swap[\"winner_model_b\"] = data_swap[\"winner_model_b\"], data_swap[\"winner_model_a\"]\n",
        "        ret.extend([proc.build_input(data_swap) for proc in processor])\n",
        "\n",
        "    dataset0.append(ret)\n",
        "\n",
        "# Now dataset0 contains the processed data\n",
        "dataset0"
      ],
      "metadata": {
        "_uuid": "7684c102-3429-4221-bdeb-75cfd3cfb9ba",
        "_cell_guid": "511568bc-4a60-4451-8fcb-91f0fb548a89",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:42.962003Z",
          "iopub.execute_input": "2025-02-23T00:03:42.962427Z",
          "iopub.status.idle": "2025-02-23T00:03:43.05157Z",
          "shell.execute_reply.started": "2025-02-23T00:03:42.962389Z",
          "shell.execute_reply": "2025-02-23T00:03:43.050559Z"
        },
        "_kg_hide-output": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "t-x9970Sy669"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize processors\n",
        "processor = [\n",
        "    ProcessorPAB(tokenizer=tokenizer, max_length=max_length, support_system_role=False),\n",
        "    ProcessorPAPB(tokenizer=tokenizer, max_length=max_length // 2, support_system_role=False)\n",
        "]\n",
        "\n",
        "# Load the DataFrame\n",
        "# df = pd.read_csv(\"../output/dtrainval.csv\")\n",
        "df = external_data\n",
        "include_swap = True\n",
        "\n",
        "# Apply the query\n",
        "query = f\"fold != {fold}\"\n",
        "# df = df.query(query).reset_index(drop=True)\n",
        "\n",
        "# Process each sample\n",
        "dataset1 = []\n",
        "for idx in range(len(df[0:20])):\n",
        "    data = df.iloc[idx].to_dict()\n",
        "    ret = [proc.build_input(data) for proc in processor]\n",
        "\n",
        "    if include_swap:\n",
        "        data_swap = copy.deepcopy(data)\n",
        "        data_swap[\"model_a\"], data_swap[\"model_b\"] = data_swap[\"model_b\"], data_swap[\"model_a\"]\n",
        "        data_swap[\"response_a\"], data_swap[\"response_b\"] = data_swap[\"response_b\"], data_swap[\"response_a\"]\n",
        "        data_swap[\"winner_model_a\"], data_swap[\"winner_model_b\"] = data_swap[\"winner_model_b\"], data_swap[\"winner_model_a\"]\n",
        "        ret.extend([proc.build_input(data_swap) for proc in processor])\n",
        "\n",
        "    dataset1.append(ret)\n",
        "\n",
        "# Now dataset1 contains the processed data\n",
        "dataset1"
      ],
      "metadata": {
        "_uuid": "b9dacfdb-127a-4c55-b8f8-6b4d02360129",
        "_cell_guid": "2fdf2809-34f8-46be-b534-b41478c85f1e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:43.052536Z",
          "iopub.execute_input": "2025-02-23T00:03:43.052797Z",
          "iopub.status.idle": "2025-02-23T00:03:43.101797Z",
          "shell.execute_reply.started": "2025-02-23T00:03:43.052774Z",
          "shell.execute_reply": "2025-02-23T00:03:43.100826Z"
        },
        "_kg_hide-output": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "NIoycyxHy66-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(dataset0[0][0].keys()))\n",
        "print(len(dataset0[0][0]['input_ids']))\n",
        "print(len(dataset1[0][0]['input_ids']))"
      ],
      "metadata": {
        "_uuid": "6263b0ee-622b-487b-9a48-3e7f42bc71bb",
        "_cell_guid": "58c758ae-9d5c-491e-9182-5a38a329e006",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:43.102995Z",
          "iopub.execute_input": "2025-02-23T00:03:43.103314Z",
          "iopub.status.idle": "2025-02-23T00:03:43.109181Z",
          "shell.execute_reply.started": "2025-02-23T00:03:43.103288Z",
          "shell.execute_reply": "2025-02-23T00:03:43.108333Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "DdhZLJ40y66-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = torch.utils.data.ConcatDataset([dataset0, dataset1])\n",
        "dataset"
      ],
      "metadata": {
        "_uuid": "43300845-ccde-42fa-aef1-45267485b86e",
        "_cell_guid": "fd451dc1-c106-4ca3-a6b2-7c4ea7d7bcbb",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:43.110113Z",
          "iopub.execute_input": "2025-02-23T00:03:43.110442Z",
          "iopub.status.idle": "2025-02-23T00:03:43.12859Z",
          "shell.execute_reply.started": "2025-02-23T00:03:43.110417Z",
          "shell.execute_reply": "2025-02-23T00:03:43.12718Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "IvKSmWnvy66_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Laoder**\n",
        "\n",
        "![image.png](attachment:4160729e-b7c9-403c-85b3-4e7bfeacac84.png)\n",
        "\n",
        "\n",
        "To get the `DataLoader` we first need data samples collating to batch , and all other arugments like `batch_size`, `drop_last`, `batch_sampler`.\n",
        "\n",
        "> When automatic batching is enabled, collate_fn is called with a list of data samples at each time. It is expected to collate the input samples into a batch for yielding from the data loader iterator. The rest of this section describes the behavior of the default collate_fn (default_collate()).\n",
        "\n",
        "\n",
        "In PyTorch's `DataLoader`, the collate function is applied after the individual samples are fetched from the dataset and grouped into a batch (as a list of samples). Here's the step-by-step process:\n",
        "\n",
        "1. **Initialize the Dataset and DataLoader:**\n",
        "The DataLoader loads a map-style dataset (e.g., torch.utils.data.Dataset), which provides access to individual samples by index.\n",
        "\n",
        "2. **Sampling:**\n",
        "The DataLoader uses a sampler (e.g., `RandomSampler`) to select indices from the dataset. These indices are grouped into batches by a batch sampler.\n",
        "\n",
        "3. **Fetch Samples:**\n",
        "For each batch of indices, the DataLoader fetches the corresponding individual samples from the dataset (e.g., dataset[0], dataset[1], etc.) and collects them into a list (e.g., `[sample0, sample1, sample2]`).\n",
        "\n",
        "4. **Collate Function:**\n",
        "The collate function (`collate_fn`) is then applied to this list of samples to merge them into a batched structure (e.g., padding sequences, stacking tensors into a batch dimension). The collate function is responsible for converting the list of samples into a batch format suitable for the model (e.g., `torch.stack()` for fixed-size tensors or `pad_sequence()` for variable-length sequences).\n",
        "Example Workflow:"
      ],
      "metadata": {
        "_uuid": "45c66419-48cf-4d3b-9fea-62b2db807374",
        "_cell_guid": "81056922-ce6a-40b1-895c-3521fa707bf8",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "RbROEx55y66_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.utils.data.DataLoader(dataset,\n",
        "#         shuffle=training,\n",
        "#         batch_size=batch_size,\n",
        "#         num_workers=num_workers,\n",
        "#         drop_last=training,\n",
        "#         collate_fn=ShardedMaxTokensCollator(\n",
        "#             max_tokens=max_tokens,\n",
        "#             base_collator=VarlenCollator(),\n",
        "#             sort_samples=training,\n",
        "#         ),\n",
        "#     )"
      ],
      "metadata": {
        "_uuid": "90dead92-d02a-4ac9-a199-04ea0cb20a58",
        "_cell_guid": "79525668-2d14-4d6a-b043-d8555d637015",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:43.12966Z",
          "iopub.execute_input": "2025-02-23T00:03:43.129948Z",
          "iopub.status.idle": "2025-02-23T00:03:43.146383Z",
          "shell.execute_reply.started": "2025-02-23T00:03:43.129921Z",
          "shell.execute_reply": "2025-02-23T00:03:43.145387Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "6RVzaLlmy67A"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size =80\n",
        "num_workers=4\n",
        "training=True\n",
        "\n",
        "max_tokens = 1024 * 16"
      ],
      "metadata": {
        "_uuid": "cca1653e-0f91-4eda-a25f-73a5bc74342e",
        "_cell_guid": "cdf24fdb-92f5-49b5-957d-7cc2293dc280",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:03:43.147383Z",
          "iopub.execute_input": "2025-02-23T00:03:43.147712Z",
          "iopub.status.idle": "2025-02-23T00:03:43.164343Z",
          "shell.execute_reply.started": "2025-02-23T00:03:43.147677Z",
          "shell.execute_reply": "2025-02-23T00:03:43.163377Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "B9yPfmf0y67A"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "# Manually set environment variables\n",
        "os.environ['RANK'] = '1'\n",
        "os.environ['WORLD_SIZE'] = '2'\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '12345'\n",
        "\n",
        "# Initialize the distributed environment\n",
        "dist.init_process_group(backend='nccl', init_method='tcp://localhost:12345', rank=0, world_size=1)\n",
        "\n",
        "# Get the rank and world size\n",
        "rank = dist.get_rank()\n",
        "world_size = dist.get_world_size()"
      ],
      "metadata": {
        "_uuid": "c393b53c-9ba0-48b0-bc99-b57c5a4584cf",
        "_cell_guid": "e35a6dc6-9741-4535-b69d-ef69e7259d9b",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "hs5m-2ycy67B"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `world_size`:\n",
        "The total number of processes (e.g., GPUs or machines) participating in distributed training.\n",
        "Role: Determines how data is partitioned into shards, ensuring each process handles an equal portion of the dataset.\n",
        "\n",
        "2. `rank`:\n",
        "A unique identifier for each process within a distributed setup `(ranges from 0 to world_size - 1)`.\n",
        "Role: Identifies which shard (data partition) a process should work on. For example, `rank=2` in a `world_size=4` setup handles the third data shard.\n",
        "\n",
        "3. `shard`:\n",
        "A logical division of the dataset assigned to a specific process (identified by `rank`).\n",
        "Role: Enables parallel processing by ensuring each process only handles its own shard, reducing memory and computation overhead."
      ],
      "metadata": {
        "_uuid": "0259eaab-bee4-4ec8-b932-9972dda4a889",
        "_cell_guid": "3cd6f51d-22a6-4a05-b16a-fd4ae8f53794",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "TLGg06yEy67K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Sharding the Dataset**:\n",
        "```python\n",
        "sample_index_matrix = torch.arange(len(samples)).reshape(-1, self.world_size)\n",
        "size_matrix = torch.tensor([sample[\"input_ids\"].size(0) for sample in samples]).reshape(-1, self.world_size)\n",
        "```\n",
        "- `sample_index_matrix`: Reshapes indices of all samples into a matrix with world_size columns. Each column represents a shard (e.g., world_size=2 → `n x 2` matrix).\n",
        "- `size_matrix`: Corresponding token counts for each sample, reshaped to align with sample_index_matrix.\n",
        "\n",
        "2. **Token-Aware Micro-Batch Splitting**:\n",
        "```python\n",
        "micro_batch_segments = []\n",
        "start = 0\n",
        "for end in range(size_matrix.size(0)):\n",
        "    if (size_matrix[start : end + 1].sum(0) > self.max_tokens).any() and end > start:\n",
        "        micro_batch_segments.append((start, end))\n",
        "        start = end\n",
        "if start < size_matrix.size(0):\n",
        "    micro_batch_segments.append((start, size_matrix.size(0)))\n",
        "```\n",
        "- **Greedy Chunking**: Segments the token data (size_matrix) into chunks (rows) where each chunk's cumulative tokens per shard (column) ≤ `self.max_tokens`.\n",
        "- **Edge Handling**: Ensures the last segment is included even if it’s smaller than the max token limit.\n",
        "\n",
        "3. **Extracting Micro-Batches for Each Process**:\n",
        "```python\n",
        "micro_batches = []\n",
        "for start, end in micro_batch_segments:\n",
        "    micro_batch_samples = [samples[i] for i in sample_index_matrix[start:end, self.rank]]\n",
        "    micro_batches.extend(self.base_collator(micro_batch_samples))\n",
        "```\n",
        "- **Per-Rank Selection**: For each segment, selects samples from the current process’s shard (column) using sample_index_matrix `[start:end, self.rank]`.\n",
        "- **Collation**: Preprocesses batches (e.g., padding tokens) for model compatibility via `self.base_collator`."
      ],
      "metadata": {
        "_uuid": "3fdbb70b-4193-466b-b704-ce75f4941775",
        "_cell_guid": "2685eeb1-a9e9-46bc-9126-721f7699f63b",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "WDWjrnXOy67K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "samples = [\n",
        "{\"input_ids\":  torch.tensor([3, 4]), \"label\": 1},\n",
        "{\"input_ids\": torch.tensor([5, 6, 7, 8]), \"label\": 0},\n",
        "{\"input_ids\": torch.tensor([9, 10, 11]), \"label\": 1},\n",
        "{\"input_ids\": torch.tensor([12, 13]), \"label\": 0},\n",
        "{\"input_ids\": torch.tensor([14, 15, 16, 17, 18]), \"label\": 1}\n",
        "]\n",
        "sort_samples = True\n",
        "max_tokens = 10\n",
        "world_size = 2 # (2 processes)\n",
        "rank = 0 # then 1 on paralllel one\n",
        "\n",
        "print(f\"Rank: {rank}, World Size: {world_size}\\n\\n\")\n",
        "\n",
        "\n",
        "# flatten if a 'sample' is a list of samples\n",
        "if isinstance(samples[0], list):\n",
        "    samples = [sample for s in samples for sample in s]\n",
        "print(f'flatten samples {samples}\\n\\n')\n",
        "\n",
        "# sorting on basis of size of input_ids within sample\n",
        "if sort_samples:\n",
        "    samples = sorted(samples, key=lambda x: x[\"input_ids\"].size(0))\n",
        "print(f'sort_samples {samples}\\n\\n')\n",
        "\n",
        "# to handle future split samples into shards with padding till following conditions is true\n",
        "while len(samples) % world_size != 0:\n",
        "    samples.append(samples[-1])\n",
        "    print(f\"Padding samples to make them divisible by num_shards={world_size}\")\n",
        "print(f'padding {samples}\\n\\n')\n",
        "\n",
        "sample_index_matrix = torch.arange(len(samples)).reshape(-1, world_size)\n",
        "size_matrix = torch.tensor([sample[\"input_ids\"].size(0) for sample in samples]).reshape(-1, world_size)\n",
        "micro_batch_segments = []\n",
        "# (start, end) that (size_matrix[start:end].sum(0) <= max_tokens).all()\n",
        "print(f'sample_index_matrix {sample_index_matrix}\\n\\n')\n",
        "print(f'size_matrix {size_matrix}\\n\\n')\n",
        "\n",
        "start = 0\n",
        "for end in range(size_matrix.size(0)):\n",
        "    # look ahead\n",
        "    if (size_matrix[start : end + 1].sum(0) > max_tokens).any() and end > start:\n",
        "        micro_batch_segments.append((start, end))\n",
        "        start = end\n",
        "print(f'micro_batch_segments {micro_batch_segments}\\n\\n')\n",
        "\n",
        "if start < size_matrix.size(0):\n",
        "    micro_batch_segments.append((start, size_matrix.size(0)))\n",
        "print(f'micro_batch_segments {micro_batch_segments}\\n\\n')\n",
        "\n",
        "micro_batches = []\n",
        "for start, end in micro_batch_segments:\n",
        "    micro_batch_samples = [samples[i] for i in sample_index_matrix[start:end, rank]]\n",
        "    print(f'{start}, {end}, {micro_batch_samples}')\n",
        "    # samples = micro_batch_samples\n",
        "    seq_lens = []\n",
        "    cu_seqlens = [0]\n",
        "    end = 0\n",
        "    input_idss = []\n",
        "    position_idss = []\n",
        "\n",
        "    for sample in micro_batch_samples:\n",
        "        seq_len = sample[\"input_ids\"].size(0)\n",
        "        seq_lens.append(seq_len)\n",
        "        end += seq_len\n",
        "        cu_seqlens.append(end)\n",
        "        input_idss.append(sample[\"input_ids\"])\n",
        "        position_idss.append(torch.arange(seq_len))\n",
        "\n",
        "    input_ids = torch.cat(input_idss, dim=0)\n",
        "    position_ids = torch.cat(position_idss, dim=0)\n",
        "\n",
        "    data = dict(batch_size=len(micro_batch_samples),\n",
        "        input_ids=input_ids.unsqueeze(0),\n",
        "        position_ids=position_ids.unsqueeze(0),\n",
        "        seq_lens=seq_lens,\n",
        "        cu_seqlens=torch.tensor(cu_seqlens, dtype=torch.int32),\n",
        "        max_seq_len=max(seq_lens))\n",
        "    print(f\"micro_batch_samples, {micro_batch_samples}\\n\")\n",
        "    print([sample[\"label\"] for sample in micro_batch_samples])\n",
        "    # data[\"label\"] = torch.stack([sample[\"label\"] for sample in micro_batch_samples], dim=0)\n",
        "    data[\"label\"] = torch.stack([torch.tensor(sample[\"label\"]) for sample in micro_batch_samples], dim=0)\n",
        "\n",
        "\n",
        "    for key in micro_batch_samples[0]:\n",
        "        if key not in data.keys():\n",
        "            data[key] = [sample[key] for sample in micro_batch_samples]\n",
        "    micro_batches.extend([data])\n",
        "    print(f'micro_batches {micro_batches}\\n\\n')\n",
        "\n",
        "print(f'micro_batch_samples {micro_batch_samples}\\n\\n')\n",
        "print(f'micro_batches {micro_batches}\\n\\n')\n",
        "\n",
        "# Clean up\n",
        "# dist.destroy_process_group()"
      ],
      "metadata": {
        "_uuid": "1d5a86eb-ad8a-489b-a6e4-2137f68ef33c",
        "_cell_guid": "70d27077-a020-427d-9aaf-94e93bdc5c63",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:04:04.755157Z",
          "iopub.execute_input": "2025-02-23T00:04:04.755514Z",
          "iopub.status.idle": "2025-02-23T00:04:04.8008Z",
          "shell.execute_reply.started": "2025-02-23T00:04:04.755486Z",
          "shell.execute_reply": "2025-02-23T00:04:04.799809Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "zOECWn0Vy67L"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `max_tokens`: specifies the maximum number of tokens allowed in a single batch.\n",
        "\n",
        "* `max_seq_len or max_length`: specifies the maximum length of a single sequence."
      ],
      "metadata": {
        "_uuid": "3c0c0417-199d-4332-af5c-e76068e7ba40",
        "_cell_guid": "e936898b-68e9-42e0-b511-daa10ec85809",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "AFJiRaoMy67M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VAL_FOLD = 0\n",
        "max_tokens = 1024 * 16\n",
        "\n",
        "max_length = 4096"
      ],
      "metadata": {
        "_uuid": "a92bf87d-62a2-4dd2-93c7-8715cb703bb5",
        "_cell_guid": "46a6f945-7a3d-43a1-99b4-449d5400b36d",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:04:09.152454Z",
          "iopub.execute_input": "2025-02-23T00:04:09.152821Z",
          "iopub.status.idle": "2025-02-23T00:04:09.157224Z",
          "shell.execute_reply.started": "2025-02-23T00:04:09.15279Z",
          "shell.execute_reply": "2025-02-23T00:04:09.156043Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "voHK90Ccy67M"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "From text.csv i found following micro_batch!\n",
        "\n",
        "```markdown\n",
        "{\n",
        "'batch_size': 3,\n",
        "'input_ids': tensor([[     2,    106,   1645,  ...,    603, 235292,  15695]]),\n",
        "'position_ids': tensor([[   0,    1,    2,  ..., 1948, 1949, 1950]]),\n",
        "'seq_lens': [298, 687, 1951],\n",
        "'cu_seqlens': tensor([   0,  298,  985, 2936], dtype=torch.int32),\n",
        "'max_seq_len': 1951,\n",
        "'label': tensor([[1., 0., 0.],\n",
        "        [1., 0., 0.],\n",
        "        [1., 0., 0.]]),\n",
        "\n",
        "'input_text': ['<bos><start_of_turn>user\\nPlease act as an impartial judge and evaluate the quality of the responses provided by two\\nAI assistants to the user question displayed below. You should choose the assistant that\\nfollows the user’s instructions and answers the user’s question better. Your evaluation\\nshould consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\\nand level of detail of their responses. Begin your evaluation by comparing the two\\nresponses and provide a short explanation. Avoid any position biases and ensure that the\\norder in which the responses were presented does not influence your decision. Do not allow\\nthe length of the responses to influence your evaluation. Do not favor certain names of\\nthe assistants. Be as objective as possible. After providing your explanation, output your\\nfinal verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\\nif assistant B is better, and \"[[C]]\" for a tie.\\n<|The Start of Conversation between a User and two Assistants|>\\n### User:\\nI have three oranges today, I ate an orange yesterday. How many oranges do I have?\\n\\n### Assistant A:\\nYou have two oranges today.\\n\\n### Assistant B:\\nYou still have three oranges. Eating an orange yesterday does not affect the number of oranges you have today.\\n<|The End of Conversation between a User and two Assistants|><end_of_turn>\\n<start_of_turn>model\\nverdict is: [[', '<bos><start_of_turn>user\\nPlease act as an impartial judge and evaluate the quality of the responses provided by two\\nAI assistants to the user question displayed below. You should choose the assistant that\\nfollows the user’s instructions and answers the user’s question better. Your evaluation\\nshould consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\\nand level of detail of their responses. Begin your evaluation by comparing the two\\nresponses and provide a short explanation. Avoid any position biases and ensure that the\\norder in which the responses were presented does not influence your decision. Do not allow\\nthe length of the responses to influence your evaluation. Do not favor certain names of\\nthe assistants. Be as objective as possible. After providing your explanation, output your\\nfinal verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\\nif assistant B is better, and \"[[C]]\" for a tie.\\n<|The Start of Conversation between a User and two Assistants|>\\n### User:\\nYou are a mediator in a heated political debate between two opposing parties. Mr Reddy is very hung up on semantic definitions of sex and gender, and believes that women are adult human females. Meanwhile Ms Blue is extremely fluid with definitions and does not care about truth. He (Ms blue uses he/him pronouns) insists that anybody can be any gender, gametes don\\'t mean anything, and that men can get pregnant. You, Mr Goddy are tasked with helping them both find a middle ground.\\n\\n### Assistant A:\\nThank you for sharing the details of the situation. As a mediator, I understand the importance of finding a middle ground that both parties can agree upon.\\n\\nIt is important to recognize that language and definitions can be complex and can vary depending on cultural and societal norms. At the same time, it is also important to acknowledge the biological differences between males and females.\\n\\nOne possible approach to finding a middle ground could be to focus on respecting individuals\\' self-identification and autonomy while also acknowledging the biological differences between males and females. This could involve using inclusive language that reflects individuals\\' gender identities while also recognizing the importance of biological sex in certain contexts, such as medical treatment and reproductive health.\\n\\nIt may also be helpful to explore ways to create a more inclusive society where individuals of all gender identities are valued and respected. This could involve promoting education and awareness about gender diversity and creating policies and practices that support individuals\\' gender identities and expression.\\n\\nUltimately, the goal of mediation is to find a solution that meets the needs and concerns of both parties. By focusing on mutual respect, open communication, and a willingness to find common ground, it is possible to reach a resolution that allows both parties to move forward in a positive and productive way.\\n\\n### Assistant B:\\nMr Reddy and Ms Blue both have valid points in their arguments. On one hand, Mr Reddy is correct that women are adult human females, as defined by their biological and physical characteristics. However, Ms Blue is right that gender is a more fluid concept and that people can identify with any gender they choose. A compromise could be that gender is defined by an individual\\'s self-identification, while sex is defined by their biological characteristics.\\n<|The End of Conversation between a User and two Assistants|><end_of_turn>\\n<start_of_turn>model\\nverdict is: [[', '<bos><start_of_turn>user\\nPlease act as an impartial judge and evaluate the quality of the responses provided by two\\nAI assistants to the user question displayed below. You should choose the assistant that\\nfollows the user’s instructions and answers the user’s question better. Your evaluation\\nshould consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\\nand level of detail of their responses. Begin your evaluation by comparing the two\\nresponses and provide a short explanation. Avoid any position biases and ensure that the\\norder in which the responses were presented does not influence your decision. Do not allow\\nthe length of the responses to influence your evaluation. Do not favor certain names of\\nthe assistants. Be as objective as possible. After providing your explanation, output your\\nfinal verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\\nif assistant B is better, and \"[[C]]\" for a tie.\\n<|The Start of Conversation between a User and two Assistants|>\\n### User:\\nHow to initialize the classification head when I do transfer learning. For example, I have a pre-trained vision transformer on ImageNet, and now I want to finetune it on StanfordCars\\n\\n### Assistant A:\\nWhen you want to initialize the classification head for transfer learning, you can follow these steps:\\n\\n  1. Load the pre-trained model: First, load the pre-trained vision transformer model on ImageNet. You can use popular deep learning libraries like PyTorch or TensorFlow to load the model.\\n  2. Freeze the pre-trained layers: Next, you need to freeze the pre-trained layers of the model so that they don\\'t get updated during the training process. This is because the pre-trained layers have learned useful features that can be reused for the new task.\\n  3. Add a new classification head: Now, you need to add a new classification head to the model. The classification head is responsible for outputting the predicted class for a given input image. The size of the classification head depends on the number of classes in the new task. For example, if you are working on the StanfordCars dataset, which has 196 classes, you would need a classification head with 196 output neurons.\\n  4. Initialize the classification head: Initialize the weights of the classification head randomly or using pre-trained weights. This is important because the initial weights can have a significant impact on the performance of the model.\\n  5. Train the model: Finally, you can train the model on the new task using your labeled data. During training, only the weights of the classification head will be updated, while the weights of the frozen pre-trained layers will remain unchanged. This allows the model to learn task-specific features while leveraging the pre-trained features.\\n\\nNote that there are different ways to initialize the classification head, such as using Xavier initialization, Kaiming initialization, or He initialization. You can choose the initialization method that works best for your specific task and model architecture.\\n\\n### Assistant B:\\nTo initialize the classification head when performing transfer learning, follow these steps:\\n\\n1. Load the pre-trained vision transformer model that was trained on ImageNet.\\n2. Replace the existing classification head with a new one suitable for the StanfordCars dataset.\\n3. Initialize the weights of the new classification head with random values or pre-trained weights if available.\\n4. Freeze the weights of the vision transformer backbone to prevent them from being updated during training.\\n5. Train the model on the StanfordCars dataset, fine-tuning the classification head while keeping the backbone frozen.\\n\\nHere\\'s an example implementation in Python using PyTorch:\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torchvision.models as models\\n\\n# Step 1: Load pre-trained vision transformer model\\nvision_transformer = models.vit_base_patch16_224(pretrained=True)\\n\\n# Step 2: Replace classification head\\nnum_classes = 196  # Number of classes in StanfordCars dataset\\nvision_transformer.head = nn.Linear(vision_transformer.head.in_features, num_classes)\\n\\n# Step 3: Initialize weights of new classification head\\nnn.init.xavier_uniform_(vision_transformer.head.weight)\\nnn.init.constant_(vision_transformer.head.bias, 0)\\n\\n# Step 4: Freeze the weights of the backbone\\nfor param in vision_transformer.parameters():\\n    param.requires_grad = False\\n\\n# Step 5: Train the model on StanfordCars dataset\\n# ...\\n```\\n\\nIn this example, we load the pre-trained vision transformer model using `torchvision.models.vit_base_patch16_224`. We then replace the `head` (classification head) of the model with a new linear layer (`nn.Linear`) that has the appropriate number of output classes for the StanfordCars dataset.\\n\\nAfter replacing the classification head, we initialize its weights using Xavier initialization (`nn.init.xavier_uniform_`) and set its biases to zero (`nn.init.constant_`).\\n\\nFinally, we freeze the weights of the vision transformer backbone by setting `requires_grad=False` for all parameters. This ensures that only the weights of the classification head will be updated during training on the StanfordCars dataset.\\n\\n### User:\\nI want to do full finetuning\\n\\n### Assistant A:\\nIf you want to do full finetuning of the pre-trained vision transformer on the StanfordCars dataset, you can follow these steps:\\n\\n  1. Load the pre-trained model: Load the pre-trained vision transformer model on ImageNet. You can use popular deep learning libraries like PyTorch or TensorFlow to load the model.\\n  2. Remove the last classification layer: The pre-trained model typically has a last classification layer that is specific to the ImageNet dataset. To adapt the model to the StanfordCars dataset, you need to remove this layer.\\n  3. Add a new classification head: Add a new classification head to the model. The classification head is responsible for outputting the predicted class for a given input image. The size of the classification head depends on the number of classes in the StanfordCars dataset, which has 196 classes. You can add a fully connected layer with 196 output neurons as the new classification head.\\n  4. Initialize the classification head: Initialize the weights of the classification head randomly or using pre-trained weights. This is important because the initial weights can have a significant impact on the performance of the model.\\n  5. Freeze the pre-trained layers: Freeze the pre-trained layers of the model so that they don\\'t get updated during the training process. This is because the pre-trained layers have learned useful features that can be reused for the new task.\\n  6. Train the model: Finally, you can train the model on the StanfordCars dataset using your labeled data. During training, all the weights of the model will be updated, including the weights of the frozen pre-trained layers. This allows the model to learn task-specific features while leveraging the pre-trained features.\\n\\nNote that when you do full finetuning, you need to use a lower learning rate and a smaller number of training epochs compared to transfer learning, because the model has already learned a lot of useful features from the pre-trained weights. You can experiment with different learning rates and training schedules to find the best hyperparameters for your specific task and model architecture.\\n\\n### Assistant B:\\nIf you want to perform full finetuning, where both the classification head and the backbone of the vision transformer are updated during training on the StanfordCars dataset, you can omit freezing the backbone weights. Here\\'s an updated version of the code:\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torchvision.models as models\\n\\n# Step 1: Load pre-trained vision transformer model\\nvision_transformer = models.vit_base_patch16_224(pretrained=True)\\n\\n# Step 2: Replace classification head\\nnum_classes = 196  # Number of classes in StanfordCars dataset\\nvision_transformer.head = nn.Linear(vision_transformer.head.in_features, num_classes)\\n\\n# Step 3: Initialize weights of new classification head\\nnn.init.xavier_uniform_(vision_transformer.head.weight)\\nnn.init.constant_(vision_transformer.head.bias, 0)\\n\\n# Step 4: Train the model on StanfordCars dataset\\n# ...\\n```\\n\\nIn this case, you can directly train the vision transformer model on the StanfordCars dataset without freezing any weights. During training, both the weights of the classification head and the backbone will be updated.\\n\\nKeep in mind that full finetuning may require more computational resources and a larger labeled dataset compared to freezing the backbone. It can help the model learn more specific features related to the StanfordCars dataset but may also increase the risk of overfitting if the dataset is small. Monitoring the training progress and adjusting hyperparameters, such as learning rate and regularization, can help mitigate overfitting.\\n<|The End of Conversation between a User and two Assistants|><end_of_turn>\\n<start_of_turn>model\\nverdict is: [[']}\n",
        "```"
      ],
      "metadata": {
        "_uuid": "6fc42447-3b44-4f05-bbb7-a875978658c2",
        "_cell_guid": "b26bc278-f107-4844-823e-a0e79b691a41",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "H2u-ybQGy67N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "About dataset manipulation:\n",
        "```python\n",
        "max_sequence_length = 4096 # after processing data tokenize with this length on text\n",
        "\n",
        "# on dataloader\n",
        "batch_size = 80\n",
        "num_worker = 4\n",
        "max_token = 1024*16 # when collate the text with the logic of Variable length Collator and this sequence will create micro_batch, which will have multiple sequence combined keeping in mind the tokens length. visialize it above below outputs\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "details about Data loader from train file have some this features mentioned below\n",
        "```python\n",
        "\n",
        "for batch in tqdm(dataloader):\n",
        "    for micro_batch in batch:\n",
        "        # print(micro_batch)\n",
        "        print(len(micro_batch['input_ids'][0]))\n",
        "        print(len(micro_batch['position_ids'][0]))\n",
        "        print(micro_batch['cu_seqlens'])\n",
        "        print(len(micro_batch['cu_seqlens']))\n",
        "        print(len(micro_batch['input_text']))\n",
        "        print(''.join(micro_batch['input_text'][0].split()[-10:])) # last 10 words\n",
        "        print(\"------------------------------\")\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "``` markdown\n",
        "13316\n",
        "13316\n",
        "tensor([    0,  4330,  5251,  6259,  6564,  9577, 10569, 11543, 13001, 13316],\n",
        "       dtype=torch.int32)\n",
        "10\n",
        "9\n",
        "betweenaUserandtwoAssistants|><end_of_turn><start_of_turn>modelverdictis:[[\n",
        "------------------------------\n",
        "16007\n",
        "16007\n",
        "tensor([    0,  3097,  3392,  3836,  4784,  5131,  5468,  6478,  6849,  8687,\n",
        "         8992,  9545, 10015, 10286, 11698, 12612, 13436, 14668, 16007],\n",
        "       dtype=torch.int32)\n",
        "19\n",
        "18\n",
        "betweenaUserandtwoAssistants|><end_of_turn><start_of_turn>modelverdictis:[[\n",
        "------------------------------\n",
        "15886\n",
        "15886\n",
        "tensor([    0,   922,  1470,  4863,  5648,  6447,  7456,  8115,  9301, 10276,\n",
        "        10720, 11237, 12055, 12350, 13605, 13924, 14541, 15159, 15886],\n",
        "       dtype=torch.int32)\n",
        "19\n",
        "18\n",
        "betweenaUserandtwoAssistants|><end_of_turn><start_of_turn>modelverdictis:[[\n",
        "------------------------------\n",
        "4681\n",
        "4681\n",
        "tensor([   0,  500, 2058, 2876, 3201, 4681], dtype=torch.int32)\n",
        "6\n",
        "5\n",
        "betweenaUserandtwoAssistants|><end_of_turn><start_of_turn>modelverdictis:[[\n",
        "------------------------------\n",
        "15993\n",
        "15993\n",
        "tensor([    0,   296,   676,  2148,  3009,  3411,  4666,  5568,  6410,  6974,\n",
        "         7420,  7805,  8594,  9529, 10444, 10998, 11855, 12336, 14436, 14993,\n",
        "        15993], dtype=torch.int32)\n",
        "21\n",
        "20\n",
        "betweenaUserandtwoAssistants|><end_of_turn><start_of_turn>modelverdictis:[[\n",
        "------------------------------\n",
        "15929\n",
        "15929\n",
        "tensor([    0,   785,  1133,  2297,  3227,  3687,  4576,  5708,  6093,  6936,\n",
        "         7347,  9561, 11896, 12671, 15228, 15929], dtype=torch.int32)\n",
        "16\n",
        "15\n",
        "betweenaUserandtwoAssistants|><end_of_turn><start_of_turn>modelverdictis:[[\n",
        "------------------------------\n",
        "```"
      ],
      "metadata": {
        "_uuid": "28f45683-4b8e-4442-83b7-b425c4d78e46",
        "_cell_guid": "68c548ea-e024-4a83-949c-82a16cd369d0",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "bzBP-117y67N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PS. you have script prepared to download the data_loader in https://www.kaggle.com/code/vanshikagupta1136/lmsys-0805/edit"
      ],
      "metadata": {
        "_uuid": "7c5d988d-113a-453d-aebf-6d6e6cb5f997",
        "_cell_guid": "66eb7d63-3c8e-4a84-acb8-ac6fb75f958c",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "EbKbXIKPy67O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Building**\n",
        "\n",
        "- Input will be for each micro_batch in batch `model(micro_batch[\"batch_size\"], micro_batch[\"input_ids\"], micro_batch[\"cu_seqlens\"])`\n"
      ],
      "metadata": {
        "_uuid": "d0bc6791-95f6-446a-ad6b-30f4fcbf4fca",
        "_cell_guid": "9a7118d3-5660-4040-822a-c948efa81dcb",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "2fmMOp5ny67O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Config**"
      ],
      "metadata": {
        "id": "xHcBM7Sfy67O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gemma2 model config\n",
        "\n",
        "vocab_size=256000,\n",
        "hidden_size=2304,\n",
        "intermediate_size=9216,\n",
        "num_hidden_layers=26,\n",
        "num_attention_heads=8,\n",
        "num_key_value_heads=4,\n",
        "head_dim=256,\n",
        "hidden_activation=\"gelu_pytorch_tanh\",\n",
        "max_position_embeddings=8192,\n",
        "initializer_range=0.02,\n",
        "rms_norm_eps=1e-6,\n",
        "use_cache=True,\n",
        "pad_token_id=0,\n",
        "eos_token_id=1,\n",
        "bos_token_id=2,\n",
        "tie_word_embeddings=True,\n",
        "rope_theta=10000.0,\n",
        "attention_bias=False,\n",
        "attention_dropout=0.0,\n",
        "query_pre_attn_scalar=256,\n",
        "sliding_window=4096,\n",
        "final_logit_softcapping=30.0,\n",
        "attn_logit_softcapping=50.0,\n",
        "cache_implementation=\"hybrid\","
      ],
      "metadata": {
        "_uuid": "4b72da97-b924-4149-bdb4-846d3b3bea77",
        "_cell_guid": "efa5ec83-b729-4810-a732-616fed6f4ff2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:04:14.112997Z",
          "iopub.execute_input": "2025-02-23T00:04:14.113399Z",
          "iopub.status.idle": "2025-02-23T00:04:14.119577Z",
          "shell.execute_reply.started": "2025-02-23T00:04:14.113366Z",
          "shell.execute_reply": "2025-02-23T00:04:14.118434Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "hekBmkuTy67O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemma2PreTrainedModel**\n",
        "\n",
        "- 2 modules mainly needs to be aware with is `Pretrainedmodel()` from transformers library and `nn.module` from pytorch library"
      ],
      "metadata": {
        "_uuid": "48216e1c-2ab5-439d-bc9b-b52d3386d304",
        "_cell_guid": "61cc620f-9d45-4dd2-bdcb-39453e4c2979",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Biqlis5Py67P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "base_model_prefix = \"model\"\n",
        "supports_gradient_checkpointing = True\n",
        "_no_split_modules = [\"Gemma2DecoderLayer\"]\n",
        "_skip_keys_device_placement = [\"past_key_values\"]\n",
        "_supports_flash_attn_2 = True\n",
        "_supports_sdpa = True\n",
        "_supports_cache_class = False\n",
        "_supports_quantized_cache = False\n",
        "_supports_static_cache = True\n",
        "_is_stateful = True\n",
        "\n",
        "\n",
        "# _init_weights for both type of module, in a gaussian distribution\n",
        "# from gemma2config\n",
        "std =  0.2 # initializer_range\n",
        "\n",
        "# i added whichever module we have nn.Linear or nn.Embedding\n",
        "module = nn.Linear(10, 30)\n",
        "if isinstance(module, nn.Linear):\n",
        "    module.weight.data.normal_(mean=0.0, std=std)\n",
        "    if module.bias is not None:\n",
        "        module.bias.data.zero_()\n",
        "\n",
        "elif isinstance(module, nn.Embedding):\n",
        "    module.weight.data.normal_(mean=0.0, std=std)\n",
        "    if module.padding_idx is not None:\n",
        "        module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "module"
      ],
      "metadata": {
        "_uuid": "7587a201-06ed-448a-90ba-800be2459451",
        "_cell_guid": "6ef644fb-b2fc-440e-8772-781883377490",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:04:15.196961Z",
          "iopub.execute_input": "2025-02-23T00:04:15.197374Z",
          "iopub.status.idle": "2025-02-23T00:04:15.210225Z",
          "shell.execute_reply.started": "2025-02-23T00:04:15.197341Z",
          "shell.execute_reply": "2025-02-23T00:04:15.209177Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "mZ47vwc-y67P"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: TO run model till we get input_ids on real dataset setup"
      ],
      "metadata": {
        "_uuid": "d0f6bb10-fdf3-4616-9581-8ce202cdda3a",
        "_cell_guid": "931e4fd3-8d3c-4aca-b4d2-46314ed15467",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "FqB3jR06y67P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now Gemma2 model have 2 components which is using to design architecture\n",
        "    - **Gemma2DecoderLayer** for multiple layers\n",
        "    - **Gemma2RMSNorm** for normalization"
      ],
      "metadata": {
        "_uuid": "e5603d5d-b282-40c4-817b-140e17248b30",
        "_cell_guid": "905d6c52-87f4-4619-adc2-7e90db33d36f",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "PmNxjNHHy67P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In nn.Embedding class:\n",
        "\n",
        "`padding_idx` (int, optional) – If specified, the entries at padding_idx do not contribute to the gradient; therefore, the embedding vector at padding_idx is not updated during training, i.e. it remains as a fixed “pad”. For a newly constructed Embedding, the embedding vector at padding_idx will default to all zeros, but can be updated to another value to be used as the padding vector.\n",
        "\n",
        "- Benefit: Padding tokens are often used to ensure that all sequences in a batch have the same length. By specifying padding_idx, the embedding layer can ignore these padding tokens during the forward pass, which can lead to computational savings."
      ],
      "metadata": {
        "_uuid": "9469a0b1-7df3-467a-8d30-801054cb12dc",
        "_cell_guid": "bd224eaf-b16a-467f-841c-1fff69458ced",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Y4MbmJ9ay67Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# for creating sample model\n",
        "input_id = torch.randint(0, 10000, (1020,)) # in model design 16 * 1024 is max of sequence of tokens # (T)\n",
        "input_id = input_id.unsqueeze(0)\n",
        "print(input_id)\n",
        "\n",
        "vocab_size = 256000 # from gemma2 config\n",
        "padding_idx = 0 # from gemma2 config\n",
        "hidden_size = 2304\n",
        "\n",
        "input_id = input_id.squeeze(0)\n",
        "print(input_id)\n",
        "embed_tokens = nn.Embedding(num_embeddings = vocab_size, embedding_dim = hidden_size, padding_idx = padding_idx) #(V,C)\n",
        "inputs_embeds = embed_tokens(input_id)\n",
        "print(inputs_embeds) # (T,C)"
      ],
      "metadata": {
        "_uuid": "be871da2-6ca6-4400-a733-dda222cba2d6",
        "_cell_guid": "f5c49183-809f-4d09-a5fb-aeb1fe4f6002",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:04:20.107287Z",
          "iopub.execute_input": "2025-02-23T00:04:20.107663Z",
          "iopub.status.idle": "2025-02-23T00:04:25.97098Z",
          "shell.execute_reply.started": "2025-02-23T00:04:20.107631Z",
          "shell.execute_reply": "2025-02-23T00:04:25.969874Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "F0K7zrAVy67Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs_embeds.shape) #(T, C)"
      ],
      "metadata": {
        "_uuid": "096201d2-d87d-4339-92c1-bcb57d21350a",
        "_cell_guid": "c3d2f915-c2f1-4508-aab4-0f617e5c315c",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:04:25.972405Z",
          "iopub.execute_input": "2025-02-23T00:04:25.972744Z",
          "iopub.status.idle": "2025-02-23T00:04:25.977592Z",
          "shell.execute_reply.started": "2025-02-23T00:04:25.972708Z",
          "shell.execute_reply": "2025-02-23T00:04:25.976451Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "AkIHi6POy67Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding Normalized**"
      ],
      "metadata": {
        "_uuid": "5abfe426-ca04-46ab-a335-f0e6bd1d75f8",
        "_cell_guid": "36953ea2-9d20-4013-92cf-369a44c8591e",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "McIfTQRRy67R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_states = inputs_embeds\n",
        "\n",
        "print(hidden_size**0.5)\n",
        "normalizer = torch.tensor(hidden_size**0.5, dtype=hidden_states.dtype)\n",
        "hidden_states = hidden_states * normalizer\n",
        "normalizer"
      ],
      "metadata": {
        "_uuid": "57c80149-0db3-4435-b905-694d3af65fff",
        "_cell_guid": "d94eff83-96c5-4aa7-a8e1-425dd237ec60",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:04:25.979055Z",
          "iopub.execute_input": "2025-02-23T00:04:25.979415Z",
          "iopub.status.idle": "2025-02-23T00:04:26.006213Z",
          "shell.execute_reply.started": "2025-02-23T00:04:25.979391Z",
          "shell.execute_reply": "2025-02-23T00:04:26.004998Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "KOjxU7mFy67R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rotary Embedding**"
      ],
      "metadata": {
        "_uuid": "08eed124-fec2-40cd-871c-43e164ef5294",
        "_cell_guid": "414463cd-8e75-48a2-81cd-2cbd068831c5",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "SBzsV9Khy67R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "head_dim = 256\n",
        "max_position_embeddings = 8192\n",
        "dim = head_dim\n",
        "inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
        "seq = torch.arange(max_position_embeddings, dtype=inv_freq.dtype)\n",
        "freqs = torch.einsum(\"i , j -> i j\", seq, inv_freq)\n",
        "emb = torch.cat((freqs, freqs), dim=-1)\n",
        "emb = emb.reshape(emb.size(0), 1, 1, emb.size(1))\n",
        "rotary_emb = emb\n",
        "print(rotary_emb)\n",
        "print(rotary_emb.shape)\n",
        "print(len(rotary_emb))"
      ],
      "metadata": {
        "_uuid": "dcd0cc2f-c5b0-4e84-ace5-773f36f4b9b5",
        "_cell_guid": "553d99fc-6674-40fc-84b6-de7046278eaf",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:04:26.007557Z",
          "iopub.execute_input": "2025-02-23T00:04:26.007943Z",
          "iopub.status.idle": "2025-02-23T00:04:26.034516Z",
          "shell.execute_reply.started": "2025-02-23T00:04:26.007901Z",
          "shell.execute_reply": "2025-02-23T00:04:26.033228Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "2-Y7eYuLy67R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- simple implementation in transformer gemma2 model\n",
        "\n",
        "```python\n",
        "query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
        "\n",
        "# position_embeddings # from Gemma2RotaryEmbedding\n",
        "# \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
        "\n",
        "# Args:\n",
        "#     q (`torch.Tensor`): The query tensor.\n",
        "#     k (`torch.Tensor`): The key tensor.\n",
        "#     cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
        "#     sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
        "#     position_ids (`torch.Tensor`, *optional*):\n",
        "#         Deprecated and unused.\n",
        "#     unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
        "#         The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
        "#         sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
        "#         that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
        "#         k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
        "#         cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
        "#         the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
        "# Returns:\n",
        "#     `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
        "# \"\"\"\n",
        "\n",
        "# cos, sin = position_embeddings\n",
        "\n",
        "# q, k, cos, sin, position_ids=None, unsqueeze_dim=1\n",
        "# cos = cos.unsqueeze(unsqueeze_dim)\n",
        "# sin = sin.unsqueeze(unsqueeze_dim)\n",
        "# q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "# k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "\n",
        "```\n",
        "- optimized implementation in transformer-engine gemma2 model\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "query_states = te.attention.FusedRoPEFunc.apply(\n",
        "            query_states, rotary_emb, \"thd\", cu_seqlens\n",
        "        )\n",
        "        key_states = te.attention.FusedRoPEFunc.apply(\n",
        "            key_states, rotary_emb, \"thd\", cu_seqlens\n",
        "        )\n",
        "```\n",
        "\n",
        "- [RoPE: From Understanding to Implementation](https://zhuanlan.zhihu.com/p/17797971938)\n",
        "- Transformer upgrade path: 2. Rotary position encoding : https://spaces.ac.cn/archives/8265\n",
        "- video [How Rotary Position Embedding Supercharges Modern LLMs](https://www.youtube.com/watch?v=SMBkImDWOyQ) good explanations and blogs : [Rotary Position Encoding](https://aakashkumarnain.github.io/posts/ml_dl_concepts/rope)"
      ],
      "metadata": {
        "_uuid": "956134d4-b084-4f70-a3bd-333ca6d88f1d",
        "_cell_guid": "d28605e2-ab3f-4c69-9514-41d16b706773",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "EszMazLJy67S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemma2DecoderLayer**\n",
        "- Gemma2SdpaAttention\n",
        "- Gemma2MLP\n",
        "- Gemma2RGMSNorm"
      ],
      "metadata": {
        "_uuid": "ab37e034-7480-468a-bbc8-03a282f7ac58",
        "_cell_guid": "c97a97f8-ad6e-4c4b-9735-f6f4145ec327",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Af9uDv4Ry67S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemma2Attention**"
      ],
      "metadata": {
        "_uuid": "9eebc343-db2c-4b82-b48f-93c4775f8740",
        "_cell_guid": "be5387a4-e5b5-4754-b970-cc6be693b62d",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "3zy6TywJy67S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![image.png](attachment:d2278852-3bc1-475f-9787-85c107abf9ab.png) -->\n",
        "chekc out attention image from gemma2 model for attentions\n",
        "\n",
        "**Multi Query Attention Head**\n",
        "- Blog [Multi Query Attention is all you need](https://fireworks.ai/blog/multi-query-attention-is-all-you-need)\n",
        "\n",
        "-\n"
      ],
      "metadata": {
        "_uuid": "a47d6d51-5739-4dae-a1dd-db58a31fa697",
        "_cell_guid": "07ca13b8-0099-4377-b872-604dfab6260a",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "KF1rIEPLy67S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_hidden_layers=26\n",
        "is_last_decoder_layer = False\n",
        "head_dim=256 # 32 * 4 * 2\n",
        "attention_bias = False\n",
        "num_heads = 8\n",
        "num_key_value_heads = 4\n",
        "hidden_size = 2304\n",
        "\n",
        "q_len, _ = hidden_states.size() # T\n",
        "# q_proj :\n",
        "q_proj = nn.Linear(in_features = hidden_size, out_features = num_heads * head_dim, bias = attention_bias)\n",
        "k_proj = nn.Linear(in_features = hidden_size, out_features = num_key_value_heads * head_dim, bias = attention_bias)\n",
        "v_proj = nn.Linear(in_features = hidden_size, out_features = num_key_value_heads * head_dim, bias = attention_bias)\n",
        "o_proj = nn.Linear(in_features = hidden_size, out_features = num_heads * head_dim, bias = attention_bias)\n",
        "\n",
        "print(f\"q_proj : {q_proj}, k_proj : {k_proj}, v_proj : {v_proj}, o_proj : {o_proj}\\n\\n\")\n",
        "\n",
        "query_states = q_proj(hidden_states)\n",
        "key_states = k_proj(hidden_states)\n",
        "value_states = v_proj(hidden_states)\n",
        "\n",
        "# T, head*dim   | T, kvhead*dim  |  T, kvhead*dim\n",
        "print(f\"query_states : {query_states.shape}, key_states : {key_states.shape}, value_states : {value_states.shape}\\n\\n\")\n",
        "\n",
        "\n",
        "query_states = query_states.view(q_len, num_heads, head_dim)\n",
        "key_states = key_states.view(q_len, num_key_value_heads, head_dim)\n",
        "value_states = value_states.view(q_len, num_key_value_heads, head_dim)\n",
        "\n",
        "print(f\"query_states : {query_states.shape}, key_states : {key_states.shape}, value_states : {value_states.shape}\\n\\n\")\n",
        "# print(f\"query_states : {query_states}, key_states : {key_states}, value_states : {value_states}\\n\\n\")"
      ],
      "metadata": {
        "_uuid": "83eed91b-9770-422f-bf4c-1b61ca518f66",
        "_cell_guid": "698a01cb-21d3-4c02-ae5d-a2f4c7524e4b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-23T00:04:35.526971Z",
          "iopub.execute_input": "2025-02-23T00:04:35.527356Z",
          "iopub.status.idle": "2025-02-23T00:04:35.844794Z",
          "shell.execute_reply.started": "2025-02-23T00:04:35.527325Z",
          "shell.execute_reply": "2025-02-23T00:04:35.84385Z"
        },
        "scrolled": true,
        "_kg_hide-output": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ISm2I-44y67T"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am gonna complete all layers of this decoder layers code today night"
      ],
      "metadata": {
        "_uuid": "27acc96a-974e-467b-aa08-74351601a5e7",
        "_cell_guid": "467feb9e-4e7e-4694-82a5-a772d216309d",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "62JkSZSAy67T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flash Attention**\n",
        "\n",
        "- [Packing with Flash Attention for Efficient Fine-tuning of LLM](https://ho.lc/blog/packing_with_flash_attention)\n",
        "    - contain concepts of flash attentions & varibale length"
      ],
      "metadata": {
        "_uuid": "344b028c-2d35-463f-8d86-cd2ba83be050",
        "_cell_guid": "1bb5dad5-2598-4f2e-8a0f-e257248e10ab",
        "trusted": true,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "bipOMPRoy67T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Above i considered layer_index = 1 for understanding\n",
        "# for i in range(num_hidden_layers):\n",
        "\n",
        "# Gemma2Decoder > P Rotation > Attention > MLP > RMSNorm"
      ],
      "metadata": {
        "_uuid": "f82ee5fb-a926-4d6a-beb3-54490de935b3",
        "_cell_guid": "6e3c1b50-1b83-415a-9b4a-3baf71b79eca",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "FYv_lSBxy67T"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuration to Run and Evaluate Model**"
      ],
      "metadata": {
        "id": "93oCSHh8y67T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FSDP**\n",
        "\n",
        " 1. **FSDP (Fully Sharded Data Parallel)**\n",
        "FSDP is a PyTorch module designed to scale model training across multiple GPUs by sharding (splitting) the model parameters and gradients across devices. This approach helps in efficiently using memory and computation, enabling the training of very large models that would otherwise not fit into a single GPU's memory.\n",
        "\n",
        " 2. **Parameters of FSDP**\n",
        "\n",
        " **`model`**\n",
        "This is the neural network model you want to wrap with FSDP. It can be any PyTorch `nn.Module`. When you pass this model to FSDP, FSDP will take care of sharding the model's parameters and gradients across the available GPUs.\n",
        "\n",
        " **`auto_wrap_policy`**\n",
        "This parameter specifies the policy for automatically wrapping layers within the model with FSDP. The `auto_wrap_policy` can be a function or a predefined policy that determines which layers should be wrapped with FSDP. This is useful for large models where you want to shard only certain layers to optimize memory usage and performance.\n",
        "\n",
        "- **How it works**: FSDP can automatically wrap sub-modules of the model based on certain criteria (e.g., module type, parameter count). This allows for fine-grained control over which parts of the model are sharded. For example, you might want to shard large layers like transformers but not small layers like batch normalization.\n",
        "    \n",
        "\n",
        " **`sharding_strategy`**\n",
        "\n",
        "This parameter specifies the strategy for sharding the model parameters. PyTorch provides several sharding strategies:\n",
        "\n",
        "- **`ShardStrategy.FULL_SHARD`**: Shards the entire model across all devices. This means that each GPU will hold a portion of the model's parameters.\n",
        "- **`ShardStrategy.SHARD_GRAD_OP`**: Shards only the gradients and optimizer states. This can be useful for reducing memory usage during the backward pass.\n",
        "- **`ShardStrategy.NO_SHARD`**: Does not shard the model parameters. This is useful for debugging or when you want to control sharding manually.\n",
        "- **How it works**: The chosen strategy determines how the model's parameters and gradients are distributed across the GPUs. For example, with `FULL_SHARD`, each GPU will only store a subset of the model's parameters, reducing the memory footprint on each device.\n",
        "    \n",
        "\n",
        " **`device_id`**\n",
        "This parameter specifies the device (GPU) on which the model will be placed. `torch.cuda.current_device()` returns the index of the current CUDA device. This ensures that the model is placed on the correct GPU in a multi-GPU setup.\n",
        "- **How it works**: In a distributed training setup, each process (rank) will typically be assigned to a specific GPU. By specifying `device_id`, you ensure that the model is correctly placed on the intended GPU, allowing for efficient data transfer and computation.\n",
        "    \n",
        "\n",
        " **`mixed_precision`**\n",
        "This parameter enables mixed precision training, which combines single-precision (FP32) and half-precision (FP16) floating-point formats to reduce memory usage and improve training speed. Mixed precision training can significantly speed up the training process while maintaining model accuracy.\n",
        "\n",
        "- **How it works**: Mixed precision training uses FP16 for most computations to save memory and speed up calculations, while critical operations (like gradient updates) are performed in FP32 to maintain numerical stability. This is typically managed using PyTorch's `torch.cuda.amp` module, which provides tools for automatic mixed precision training.\n",
        "    \n",
        "\n",
        "When you wrap a model with FSDP using this code snippet, the following happens:\n",
        "\n",
        "1. **Model Initialization**: The model is passed to FSDP, which prepares it for distributed training.\n",
        "2. **Auto Wrap Policy**: FSDP uses the specified `auto_wrap_policy` to determine which layers of the model should be wrapped and sharded across GPUs.\n",
        "3. **Sharding Strategy**: The chosen `sharding_strategy` dictates how the model's parameters and gradients are distributed across the GPUs.\n",
        "4. **Device Placement**: The model is placed on the specified `device_id`, ensuring it runs on the correct GPU.\n",
        "5. **Mixed Precision**: Mixed precision training is enabled, allowing the model to use both FP16 and FP32 formats to optimize memory usage and training speed.\n",
        "    \n",
        "\n",
        "This setup is particularly useful for training large models efficiently across multiple GPUs, reducing memory usage, and improving training speed."
      ],
      "metadata": {
        "id": "wx3-rKNZy67U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailing: https://pytorch.org/docs/stable/fsdp.html"
      ],
      "metadata": {
        "id": "2vLsC9pGy67U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = FSDP(\n",
        "#     model,\n",
        "#     auto_wrap_policy=fsdp.auto_wrap_policy,\n",
        "#     sharding_strategy=fsdp.sharding_strategy,\n",
        "#     device_id=torch.cuda.current_device(),\n",
        "#     mixed_precision=fsdp.mixed_precision\n",
        "# )"
      ],
      "metadata": {
        "trusted": true,
        "id": "lyS3m4hxy67U"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why We Use Gradient Clipping in FSDP\n",
        "\n",
        "*   \n",
        "\n"
      ],
      "metadata": {
        "id": "CoWexD5Ey67U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About** `dist.barrier()`\n",
        "*\n"
      ],
      "metadata": {
        "id": "PsHwjp7py67V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "About GPUs architecture :\n",
        "- Cornell [workshop](https://cvw.cac.cornell.edu/gpu-architecture/gpu-example-tesla-v100/index)\n",
        "- Comparison blogs: [Nvdia T4 vs A100](https://www.server-parts.eu/post/nvidia-t4-vs-a100-gpu-comparison-ai-deep-learning-data-centers)\n",
        "\n",
        "\n",
        "How many flops and how decide to choose GPU for learning\n",
        "\n"
      ],
      "metadata": {
        "id": "FlOH2kKBy67V"
      }
    }
  ]
}